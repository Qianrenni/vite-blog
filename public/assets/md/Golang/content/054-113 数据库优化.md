## 11.3 数据库优化

### 索引优化

#### 索引类型和选择策略
```go
package main

import (
    "fmt"
    "gorm.io/driver/mysql"
    "gorm.io/gorm"
    "time"
)

// 用户模型 - 展示不同索引类型
type User struct {
    ID          uint      `gorm:"primaryKey"`
    Name        string    `gorm:"index:idx_name"`                    // 普通索引
    Email       string    `gorm:"uniqueIndex:uniq_email"`            // 唯一索引
    Age         int       `gorm:"index:idx_age"`                     // 数值索引
    Status      string    `gorm:"index:idx_status"`                  // 状态索引
    Category    string    `gorm:"index:idx_category_status"`         // 复合索引
    SubCategory string    `gorm:"index:idx_category_status"`         // 复合索引
    CreatedAt   time.Time `gorm:"index:idx_created_at"`              // 时间索引
    UpdatedAt   time.Time
    Profile     UserProfile `gorm:"foreignKey:UserID"`               // 外键索引
}

type UserProfile struct {
    ID     uint   `gorm:"primaryKey"`
    UserID uint   `gorm:"index"`                                // 外键索引
    Bio    string `gorm:"type:text"`
}

// 订单模型 - 复杂查询场景
type Order struct {
    ID          uint      `gorm:"primaryKey"`
    UserID      uint      `gorm:"index:idx_user_created"`            // 复合索引
    ProductID   uint      `gorm:"index:idx_product"`                 // 产品索引
    Status      string    `gorm:"index:idx_status_created"`          // 复合索引
    Total       float64   `gorm:"index:idx_total"`                   // 金额索引
    CreatedAt   time.Time `gorm:"index:idx_created_at,idx_user_created,idx_status_created"` // 多复合索引
    UpdatedAt   time.Time
}

// 索引优化工具类
type IndexOptimizer struct {
    db *gorm.DB
}

func NewIndexOptimizer(db *gorm.DB) *IndexOptimizer {
    return &IndexOptimizer{db: db}
}

// 分析查询性能并建议索引
func (io *IndexOptimizer) AnalyzeQueryPerformance() {
    // 启用慢查询日志
    io.db.Debug() // 会输出所有SQL语句
    
    // 模拟慢查询场景
    var users []User
    
    // 无索引查询 - 会很慢
    startTime := time.Now()
    io.db.Where("name LIKE ?", "%张%").Find(&users)
    fmt.Printf("模糊查询耗时: %v\n", time.Since(startTime))
    
    // 有索引查询 - 会更快
    startTime = time.Now()
    io.db.Where("email = ?", "test@example.com").Find(&users)
    fmt.Printf("索引查询耗时: %v\n", time.Since(startTime))
    
    // 复合索引查询
    startTime = time.Now()
    io.db.Where("category = ? AND status = ?", "tech", "active").Find(&users)
    fmt.Printf("复合索引查询耗时: %v\n", time.Since(startTime))
}

// 索引使用建议
func (io *IndexOptimizer) IndexRecommendations() {
    recommendations := []struct {
        scenario    string
        recommendation string
        example     string
    }{
        {
            "频繁WHERE查询",
            "为WHERE子句中的列创建索引",
            "WHERE name = 'John' -> CREATE INDEX idx_name ON users(name)",
        },
        {
            "JOIN操作",
            "为JOIN条件中的列创建索引",
            "JOIN users u ON o.user_id = u.id -> CREATE INDEX idx_user_id ON orders(user_id)",
        },
        {
            "ORDER BY排序",
            "为ORDER BY子句中的列创建索引",
            "ORDER BY created_at DESC -> CREATE INDEX idx_created_at ON users(created_at)",
        },
        {
            "GROUP BY分组",
            "为GROUP BY子句中的列创建索引",
            "GROUP BY status -> CREATE INDEX idx_status ON users(status)",
        },
        {
            "复合查询",
            "创建复合索引，注意列顺序",
            "WHERE status = 'active' AND created_at > '2023-01-01' -> CREATE INDEX idx_status_created ON users(status, created_at)",
        },
    }
    
    fmt.Println("索引优化建议:")
    for _, rec := range recommendations {
        fmt.Printf("场景: %s\n", rec.scenario)
        fmt.Printf("建议: %s\n", rec.recommendation)
        fmt.Printf("示例: %s\n\n", rec.example)
    }
}

// 索引监控和统计
func (io *IndexOptimizer) MonitorIndexUsage() {
    // 在MySQL中查看索引使用情况
    var indexStats []map[string]interface{}
    io.db.Raw(`
        SELECT 
            TABLE_NAME,
            INDEX_NAME,
            CARDINALITY,
            SUB_PART,
            PACKED,
            NULLABLE,
            INDEX_TYPE
        FROM information_schema.STATISTICS 
        WHERE TABLE_SCHEMA = DATABASE()
        ORDER BY TABLE_NAME, SEQ_IN_INDEX
    `).Scan(&indexStats)
    
    fmt.Println("索引统计信息:")
    for _, stat := range indexStats {
        fmt.Printf("表: %v, 索引: %v, 基数: %v\n", 
            stat["TABLE_NAME"], stat["INDEX_NAME"], stat["CARDINALITY"])
    }
}
```

#### 索引设计最佳实践
```go
// 索引设计工具
type IndexDesigner struct {
    db *gorm.DB
}

func NewIndexDesigner(db *gorm.DB) *IndexDesigner {
    return &IndexDesigner{db: db}
}

// 自动索引分析
func (id *IndexDesigner) AnalyzeTableIndexes(tableName string) {
    // 分析表结构和查询模式
    fmt.Printf("分析表 %s 的索引使用情况...\n", tableName)
    
    // 检查缺失的索引
    missingIndexes := id.findMissingIndexes(tableName)
    fmt.Printf("建议添加的索引: %v\n", missingIndexes)
    
    // 检查冗余索引
    redundantIndexes := id.findRedundantIndexes(tableName)
    fmt.Printf("冗余索引: %v\n", redundantIndexes)
    
    // 检查低效索引
    inefficientIndexes := id.findInefficientIndexes(tableName)
    fmt.Printf("低效索引: %v\n", inefficientIndexes)
}

func (id *IndexDesigner) findMissingIndexes(tableName string) []string {
    // 基于常见查询模式建议索引
    suggestions := []string{
        "为频繁查询的字段添加索引",
        "为外键字段添加索引",
        "为排序字段添加索引",
        "考虑创建复合索引",
    }
    return suggestions
}

func (id *IndexDesigner) findRedundantIndexes(tableName string) []string {
    // 查找重复或不必要的索引
    redundant := []string{
        "检查单列索引是否被复合索引覆盖",
        "删除从未使用的索引",
        "合并功能相似的索引",
    }
    return redundant
}

func (id *IndexDesigner) findInefficientIndexes(tableName string) []string {
    // 查找选择性差的索引
    inefficient := []string{
        "状态字段上的索引（如果状态值很少）",
        "性别字段上的索引",
        "布尔字段上的索引",
    }
    return inefficient
}

// 索引效果测试
func (id *IndexDesigner) TestIndexPerformance() {
    // 创建测试数据
    id.createTestData()
    
    // 测试不同查询的性能
    id.testQueryPerformance()
}

func (id *IndexDesigner) createTestData() {
    // 创建大量测试数据来验证索引效果
    fmt.Println("创建测试数据...")
    
    // 批量插入用户数据
    users := make([]User, 10000)
    for i := 0; i < 10000; i++ {
        users[i] = User{
            Name:     fmt.Sprintf("User%d", i),
            Email:    fmt.Sprintf("user%d@example.com", i),
            Age:      18 + (i % 50),
            Status:   []string{"active", "inactive"}[i%2],
            Category: []string{"tech", "finance", "health"}[i%3],
        }
    }
    
    id.db.CreateInBatches(users, 1000)
    fmt.Println("测试数据创建完成")
}

func (id *IndexDesigner) testQueryPerformance() {
    // 测试不同查询的执行时间
    queries := []struct {
        name  string
        query func() error
    }{
        {
            "无索引查询",
            func() error {
                var users []User
                return id.db.Where("name LIKE ?", "%User123%").Find(&users).Error
            },
        },
        {
            "索引查询",
            func() error {
                var users []User
                return id.db.Where("email = ?", "user123@example.com").Find(&users).Error
            },
        },
        {
            "复合索引查询",
            func() error {
                var users []User
                return id.db.Where("category = ? AND status = ?", "tech", "active").Find(&users).Error
            },
        },
    }
    
    for _, q := range queries {
        startTime := time.Now()
        err := q.query()
        duration := time.Since(startTime)
        
        if err != nil {
            fmt.Printf("查询 %s 失败: %v\n", q.name, err)
        } else {
            fmt.Printf("查询 %s 耗时: %v\n", q.name, duration)
        }
    }
}
```

### 查询优化

#### SQL查询优化技术
```go
// 查询优化器
type QueryOptimizer struct {
    db *gorm.DB
}

func NewQueryOptimizer(db *gorm.DB) *QueryOptimizer {
    return &QueryOptimizer{db: db}
}

// 查询分析和优化
func (qo *QueryOptimizer) AnalyzeAndOptimize() {
    // 1. 避免SELECT *
    qo.optimizeSelectFields()
    
    // 2. 使用LIMIT
    qo.optimizeWithLimit()
    
    // 3. 避免N+1查询
    qo.optimizeNPlusOne()
    
    // 4. 使用JOIN替代子查询
    qo.optimizeWithJoin()
    
    // 5. 使用索引提示
    qo.optimizeWithIndexHints()
}

func (qo *QueryOptimizer) optimizeSelectFields() {
    fmt.Println("优化1: 指定查询字段")
    
    // 不好的做法 - SELECT *
    var users []User
    startTime := time.Now()
    qo.db.Find(&users)
    fmt.Printf("SELECT * 耗时: %v\n", time.Since(startTime))
    
    // 好的做法 - 指定字段
    var userSummaries []struct {
        ID    uint
        Name  string
        Email string
    }
    startTime = time.Now()
    qo.db.Select("id, name, email").Find(&userSummaries)
    fmt.Printf("指定字段查询耗时: %v\n", time.Since(startTime))
}

func (qo *QueryOptimizer) optimizeWithLimit() {
    fmt.Println("优化2: 使用LIMIT")
    
    // 不好的做法 - 获取所有数据
    var allUsers []User
    startTime := time.Now()
    qo.db.Find(&allUsers)
    fmt.Printf("无LIMIT查询耗时: %v\n", time.Since(startTime))
    
    // 好的做法 - 使用LIMIT
    var limitedUsers []User
    startTime = time.Now()
    qo.db.Limit(100).Find(&limitedUsers)
    fmt.Printf("LIMIT查询耗时: %v\n", time.Since(startTime))
}

func (qo *QueryOptimizer) optimizeNPlusOne() {
    fmt.Println("优化3: 解决N+1查询问题")
    
    // 不好的做法 - N+1查询
    var orders []Order
    qo.db.Find(&orders)
    
    startTime := time.Now()
    for _, order := range orders {
        var user User
        qo.db.First(&user, order.UserID) // 每次都查询数据库
    }
    fmt.Printf("N+1查询耗时: %v\n", time.Since(startTime))
    
    // 好的做法 - 预加载
    var ordersWithUsers []Order
    startTime = time.Now()
    qo.db.Preload("User").Find(&ordersWithUsers) // 一次性加载所有关联数据
    fmt.Printf("预加载查询耗时: %v\n", time.Since(startTime))
}

func (qo *QueryOptimizer) optimizeWithJoin() {
    fmt.Println("优化4: 使用JOIN替代子查询")
    
    // 不好的做法 - 子查询
    startTime := time.Now()
    var usersWithOrders []User
    qo.db.Where("id IN (SELECT DISTINCT user_id FROM orders WHERE status = ?)", "completed").Find(&usersWithOrders)
    fmt.Printf("子查询耗时: %v\n", time.Since(startTime))
    
    // 好的做法 - JOIN查询
    startTime = time.Now()
    qo.db.Joins("JOIN orders ON users.id = orders.user_id").
        Where("orders.status = ?", "completed").
        Group("users.id").
        Find(&usersWithOrders)
    fmt.Printf("JOIN查询耗时: %v\n", time.Since(startTime))
}

func (qo *QueryOptimizer) optimizeWithIndexHints() {
    fmt.Println("优化5: 使用索引提示")
    
    // 使用索引提示强制使用特定索引
    var users []User
    startTime := time.Now()
    qo.db.Clauses(hints.UseIndex("idx_email")).Where("email = ?", "test@example.com").Find(&users)
    fmt.Printf("使用索引提示耗时: %v\n", time.Since(startTime))
}

// 查询缓存优化
type QueryCache struct {
    cache map[string]interface{}
    ttl   time.Duration
}

func NewQueryCache(ttl time.Duration) *QueryCache {
    return &QueryCache{
        cache: make(map[string]interface{}),
        ttl:   ttl,
    }
}

func (qc *QueryCache) Get(key string) (interface{}, bool) {
    if value, exists := qc.cache[key]; exists {
        return value, true
    }
    return nil, false
}

func (qc *QueryCache) Set(key string, value interface{}) {
    qc.cache[key] = value
    // 实现过期机制
    go func() {
        time.Sleep(qc.ttl)
        delete(qc.cache, key)
    }()
}

// 批量查询优化
func (qo *QueryOptimizer) optimizeBatchQueries() {
    fmt.Println("批量查询优化")
    
    // 不好的做法 - 循环查询
    ids := []uint{1, 2, 3, 4, 5}
    startTime := time.Now()
    var users1 []User
    for _, id := range ids {
        var user User
        qo.db.First(&user, id)
        users1 = append(users1, user)
    }
    fmt.Printf("循环查询耗时: %v\n", time.Since(startTime))
    
    // 好的做法 - 批量查询
    startTime = time.Now()
    var users2 []User
    qo.db.Where("id IN ?", ids).Find(&users2)
    fmt.Printf("批量查询耗时: %v\n", time.Since(startTime))
}

// 分页查询优化
func (qo *QueryOptimizer) optimizePagination() {
    fmt.Println("分页查询优化")
    
    // 不好的做法 - OFFSET分页（大数据量时性能差）
    page := 1000
    pageSize := 20
    startTime := time.Now()
    var users1 []User
    qo.db.Offset((page-1)*pageSize).Limit(pageSize).Find(&users1)
    fmt.Printf("OFFSET分页耗时: %v\n", time.Since(startTime))
    
    // 好的做法 - 游标分页
    lastID := uint(10000)
    startTime = time.Now()
    var users2 []User
    qo.db.Where("id > ?", lastID).Limit(pageSize).Find(&users2)
    fmt.Logf("游标分页耗时: %v\n", time.Since(startTime))
}
```

#### 查询执行计划分析
```go
// 查询执行计划分析器
type QueryAnalyzer struct {
    db *gorm.DB
}

func NewQueryAnalyzer(db *gorm.DB) *QueryAnalyzer {
    return &QueryAnalyzer{db: db}
}

// 分析查询执行计划
func (qa *QueryAnalyzer) AnalyzeExecutionPlan() {
    // 使用EXPLAIN分析查询
    fmt.Println("查询执行计划分析:")
    
    queries := []string{
        "SELECT * FROM users WHERE name = 'John'",
        "SELECT * FROM orders o JOIN users u ON o.user_id = u.id WHERE u.status = 'active'",
        "SELECT COUNT(*) FROM users GROUP BY status",
    }
    
    for _, query := range queries {
        fmt.Printf("\n分析查询: %s\n", query)
        qa.explainQuery(query)
    }
}

func (qa *QueryAnalyzer) explainQuery(query string) {
    // 在MySQL中使用EXPLAIN
    var explainResults []map[string]interface{}
    explainQuery := fmt.Sprintf("EXPLAIN %s", query)
    
    err := qa.db.Raw(explainQuery).Scan(&explainResults).Error
    if err != nil {
        fmt.Printf("EXPLAIN查询失败: %v\n", err)
        return
    }
    
    fmt.Println("执行计划:")
    for _, row := range explainResults {
        fmt.Printf("  表: %v, 类型: %v, 行数: %v, 额外信息: %v\n",
            row["table"], row["type"], row["rows"], row["Extra"])
    }
}

// 查询性能监控
func (qa *QueryAnalyzer) MonitorQueryPerformance() {
    // 监控慢查询
    fmt.Println("监控慢查询...")
    
    // 查询慢查询日志
    var slowQueries []map[string]interface{}
    err := qa.db.Raw(`
        SELECT 
            DIGEST_TEXT,
            COUNT_STAR,
            AVG_TIMER_WAIT/1000000000 as AVG_TIME_MS,
            MAX_TIMER_WAIT/1000000000 as MAX_TIME_MS
        FROM performance_schema.events_statements_summary_by_digest 
        WHERE AVG_TIMER_WAIT > 1000000000000  -- 平均执行时间超过1秒
        ORDER BY AVG_TIMER_WAIT DESC
        LIMIT 10
    `).Scan(&slowQueries).Error
    
    if err != nil {
        fmt.Printf("查询慢查询统计失败: %v\n", err)
        return
    }
    
    fmt.Println("慢查询统计:")
    for _, query := range slowQueries {
        fmt.Printf("  查询: %v\n", query["DIGEST_TEXT"])
        fmt.Printf("  执行次数: %v, 平均时间: %v ms, 最大时间: %v ms\n",
            query["COUNT_STAR"], query["AVG_TIME_MS"], query["MAX_TIME_MS"])
        fmt.Println()
    }
}
```

### 连接池优化

#### 连接池配置和调优
```go
package main

import (
    "database/sql"
    "fmt"
    "log"
    "sync"
    "sync/atomic"
    "time"
    
    _ "github.com/go-sql-driver/mysql"
    "gorm.io/driver/mysql"
    "gorm.io/gorm"
)

// 连接池监控器
type ConnectionPoolMonitor struct {
    db            *sql.DB
    stats         ConnectionPoolStats
    monitorTicker *time.Ticker
    stopChan      chan struct{}
}

type ConnectionPoolStats struct {
    MaxOpenConnections int64
    OpenConnections    int64
    InUse              int64
    Idle               int64
    WaitCount          int64
    WaitDuration       time.Duration
    MaxIdleClosed      int64
    MaxLifetimeClosed  int64
}

func NewConnectionPoolMonitor(db *sql.DB) *ConnectionPoolMonitor {
    return &ConnectionPoolMonitor{
        db:       db,
        stopChan: make(chan struct{}),
    }
}

// 启动连接池监控
func (cpm *ConnectionPoolMonitor) StartMonitoring(interval time.Duration) {
    cpm.monitorTicker = time.NewTicker(interval)
    
    go func() {
        for {
            select {
            case <-cpm.monitorTicker.C:
                cpm.collectStats()
                cpm.printStats()
            case <-cpm.stopChan:
                return
            }
        }
    }()
}

// 收集连接池统计信息
func (cpm *ConnectionPoolMonitor) collectStats() {
    stats := cpm.db.Stats()
    
    cpm.stats.MaxOpenConnections = int64(stats.MaxOpenConnections)
    cpm.stats.OpenConnections = int64(stats.OpenConnections)
    cpm.stats.InUse = int64(stats.InUse)
    cpm.stats.Idle = int64(stats.Idle)
    cpm.stats.WaitCount = stats.WaitCount
    cpm.stats.WaitDuration = stats.WaitDuration
    cpm.stats.MaxIdleClosed = stats.MaxIdleClosed
    cpm.stats.MaxLifetimeClosed = stats.MaxLifetimeClosed
}

// 打印连接池统计信息
func (cpm *ConnectionPoolMonitor) printStats() {
    stats := cpm.stats
    fmt.Printf("\n=== 连接池统计信息 ===\n")
    fmt.Printf("最大打开连接数: %d\n", stats.MaxOpenConnections)
    fmt.Printf("当前打开连接数: %d\n", stats.OpenConnections)
    fmt.Printf("正在使用连接数: %d\n", stats.InUse)
    fmt.Printf("空闲连接数: %d\n", stats.Idle)
    fmt.Printf("等待连接数: %d\n", stats.WaitCount)
    fmt.Printf("总等待时间: %v\n", stats.WaitDuration)
    fmt.Printf("因空闲关闭连接数: %d\n", stats.MaxIdleClosed)
    fmt.Printf("因超时关闭连接数: %d\n", stats.MaxLifetimeClosed)
    fmt.Printf("====================\n\n")
}

// 停止监控
func (cpm *ConnectionPoolMonitor) Stop() {
    close(cpm.stopChan)
    if cpm.monitorTicker != nil {
        cpm.monitorTicker.Stop()
    }
}

// 连接池优化器
type ConnectionPoolOptimizer struct {
    db *sql.DB
}

func NewConnectionPoolOptimizer(db *sql.DB) *ConnectionPoolOptimizer {
    return &ConnectionPoolOptimizer{db: db}
}

// 优化连接池配置
func (cpo *ConnectionPoolOptimizer) OptimizePoolConfig() {
    fmt.Println("优化连接池配置...")
    
    // 获取当前配置
    currentStats := cpo.db.Stats()
    fmt.Printf("当前配置: %+v\n", currentStats)
    
    // 根据应用负载调整配置
    cpo.adjustPoolSize()
    cpo.adjustIdleConnections()
    cpo.adjustConnectionLifetime()
    
    // 验证优化效果
    cpo.verifyOptimization()
}

func (cpo *ConnectionPoolOptimizer) adjustPoolSize() {
    // 设置最大打开连接数
    maxOpenConns := 100
    cpo.db.SetMaxOpenConns(maxOpenConns)
    fmt.Printf("设置最大打开连接数: %d\n", maxOpenConns)
    
    // 设置最大空闲连接数
    maxIdleConns := 25
    cpo.db.SetMaxIdleConns(maxIdleConns)
    fmt.Printf("设置最大空闲连接数: %d\n", maxIdleConns)
}

func (cpo *ConnectionPoolOptimizer) adjustIdleConnections() {
    // 设置空闲连接的最大生命周期
    idleTimeout := 5 * time.Minute
    cpo.db.SetConnMaxIdleTime(idleTimeout)
    fmt.Printf("设置空闲连接最大生命周期: %v\n", idleTimeout)
}

func (cpo *ConnectionPoolOptimizer) adjustConnectionLifetime() {
    // 设置连接的最大生命周期
    maxLifetime := 1 * time.Hour
    cpo.db.SetConnMaxLifetime(maxLifetime)
    fmt.Printf("设置连接最大生命周期: %v\n", maxLifetime)
}

func (cpo *ConnectionPoolOptimizer) verifyOptimization() {
    // 验证配置是否生效
    stats := cpo.db.Stats()
    fmt.Printf("优化后配置: %+v\n", stats)
    
    // 模拟负载测试
    cpo.loadTest()
}

// 负载测试
func (cpo *ConnectionPoolOptimizer) loadTest() {
    fmt.Println("开始连接池负载测试...")
    
    const (
        numWorkers = 50
        numQueries = 1000
    )
    
    var wg sync.WaitGroup
    queriesPerWorker := numQueries / numWorkers
    
    startTime := time.Now()
    
    // 启动多个goroutine模拟并发查询
    for i := 0; i < numWorkers; i++ {
        wg.Add(1)
        go func(workerID int) {
            defer wg.Done()
            
            for j := 0; j < queriesPerWorker; j++ {
                var result int
                err := cpo.db.QueryRow("SELECT 1").Scan(&result)
                if err != nil {
                    log.Printf("Worker %d 查询失败: %v", workerID, err)
                }
                
                // 模拟查询处理时间
                time.Sleep(10 * time.Millisecond)
            }
        }(i)
    }
    
    wg.Wait()
    
    duration := time.Since(startTime)
    fmt.Printf("负载测试完成，耗时: %v\n", duration)
    fmt.Printf("平均每秒处理查询数: %.2f\n", float64(numQueries)/duration.Seconds())
}

// 连接池健康检查
func (cpo *ConnectionPoolOptimizer) HealthCheck() {
    fmt.Println("执行连接池健康检查...")
    
    // 检查连接是否正常
    err := cpo.db.Ping()
    if err != nil {
        fmt.Printf("连接池健康检查失败: %v\n", err)
        return
    }
    
    fmt.Println("连接池健康检查通过")
    
    // 检查连接池状态
    stats := cpo.db.Stats()
    if stats.WaitCount > 0 {
        fmt.Printf("警告: 有连接等待，可能需要增加连接池大小\n")
    }
    
    if stats.MaxIdleClosed > 0 {
        fmt.Printf("提示: 有连接因空闲被关闭，可能需要调整空闲连接设置\n")
    }
    
    if stats.MaxLifetimeClosed > 0 {
        fmt.Printf("提示: 有连接因超时被关闭，可能需要调整连接生命周期设置\n")
    }
}

// 自适应连接池调整
type AdaptiveConnectionPool struct {
    db           *sql.DB
    targetUtilization float64
    adjustmentFactor  float64
    lastAdjustment    time.Time
    mutex             sync.RWMutex
}

func NewAdaptiveConnectionPool(db *sql.DB) *AdaptiveConnectionPool {
    return &AdaptiveConnectionPool{
        db:                db,
        targetUtilization: 0.7, // 目标利用率70%
        adjustmentFactor:  0.1, // 调整因子10%
        lastAdjustment:    time.Now(),
    }
}

// 自适应调整连接池大小
func (acp *AdaptiveConnectionPool) AdaptiveAdjust() {
    acp.mutex.Lock()
    defer acp.mutex.Unlock()
    
    // 检查是否需要调整（至少间隔1分钟）
    if time.Since(acp.lastAdjustment) < time.Minute {
        return
    }
    
    stats := acp.db.Stats()
    if stats.MaxOpenConnections == 0 {
        return
    }
    
    // 计算当前利用率
    utilization := float64(stats.InUse) / float64(stats.MaxOpenConnections)
    fmt.Printf("当前连接池利用率: %.2f%%\n", utilization*100)
    
    // 根据利用率调整连接池大小
    if utilization > acp.targetUtilization+0.1 {
        // 利用率过高，增加连接数
        newMaxConns := int(float64(stats.MaxOpenConnections) * (1 + acp.adjustmentFactor))
        if newMaxConns > stats.MaxOpenConnections {
            acp.db.SetMaxOpenConns(newMaxConns)
            fmt.Printf("增加最大连接数到: %d\n", newMaxConns)
        }
    } else if utilization < acp.targetUtilization-0.1 {
        // 利用率过低，减少连接数
        newMaxConns := int(float64(stats.MaxOpenConnections) * (1 - acp.adjustmentFactor))
        if newMaxConns < stats.MaxOpenConnections && newMaxConns > 10 {
            acp.db.SetMaxOpenConns(newMaxConns)
            fmt.Printf("减少最大连接数到: %d\n", newMaxConns)
        }
    }
    
    acp.lastAdjustment = time.Now()
}
```

### 分库分表

#### 分库分表实现
```go
package main

import (
    "fmt"
    "hash/crc32"
    "strconv"
    "strings"
    "time"
    
    "gorm.io/gorm"
)

// 分库分表策略接口
type ShardingStrategy interface {
    GetDatabaseName(tableName string, shardingKey interface{}) string
    GetTableName(tableName string, shardingKey interface{}) string
}

// 哈希分片策略
type HashShardingStrategy struct {
    dbCount    int
    tableCount int
}

func NewHashShardingStrategy(dbCount, tableCount int) *HashShardingStrategy {
    return &HashShardingStrategy{
        dbCount:    dbCount,
        tableCount: tableCount,
    }
}

func (hss *HashShardingStrategy) GetDatabaseName(tableName string, shardingKey interface{}) string {
    keyStr := fmt.Sprintf("%v", shardingKey)
    hash := crc32.ChecksumIEEE([]byte(keyStr))
    dbIndex := int(hash) % hss.dbCount
    return fmt.Sprintf("db_%d", dbIndex)
}

func (hss *HashShardingStrategy) GetTableName(tableName string, shardingKey interface{}) string {
    keyStr := fmt.Sprintf("%v", shardingKey)
    hash := crc32.ChecksumIEEE([]byte(keyStr))
    tableIndex := int(hash) % hss.tableCount
    return fmt.Sprintf("%s_%d", tableName, tableIndex)
}

// 时间分片策略
type TimeShardingStrategy struct {
    timeFormat string // "200601" for monthly, "20060102" for daily
}

func NewTimeShardingStrategy(timeFormat string) *TimeShardingStrategy {
    return &TimeShardingStrategy{timeFormat: timeFormat}
}

func (tss *TimeShardingStrategy) GetDatabaseName(tableName string, shardingKey interface{}) string {
    switch key := shardingKey.(type) {
    case time.Time:
        return fmt.Sprintf("db_%s", key.Format("2006"))
    case string:
        if t, err := time.Parse(time.RFC3339, key); err == nil {
            return fmt.Sprintf("db_%s", t.Format("2006"))
        }
    }
    return "db_default"
}

func (tss *TimeShardingStrategy) GetTableName(tableName string, shardingKey interface{}) string {
    switch key := shardingKey.(type) {
    case time.Time:
        return fmt.Sprintf("%s_%s", tableName, key.Format(tss.timeFormat))
    case string:
        if t, err := time.Parse(time.RFC3339, key); err == nil {
            return fmt.Sprintf("%s_%s", tableName, t.Format(tss.timeFormat))
        }
    }
    return tableName
}

// 分库分表管理器
type ShardingManager struct {
    strategy ShardingStrategy
    dbMap    map[string]*gorm.DB // 数据库连接映射
}

func NewShardingManager(strategy ShardingStrategy, dbMap map[string]*gorm.DB) *ShardingManager {
    return &ShardingManager{
        strategy: strategy,
        dbMap:    dbMap,
    }
}

// 用户实体 - 支持分库分表
type ShardedUser struct {
    ID        uint      `gorm:"primaryKey"`
    UserID    uint      `gorm:"column:user_id"` // 分片键
    Name      string    `gorm:"column:name"`
    Email     string    `gorm:"column:email"`
    CreatedAt time.Time `gorm:"column:created_at"`
}

func (ShardedUser) TableName() string {
    return "users"
}

// 订单实体 - 支持分库分表
type ShardedOrder struct {
    ID        uint      `gorm:"primaryKey"`
    OrderID   uint      `gorm:"column:order_id"` // 分片键
    UserID    uint      `gorm:"column:user_id"`
    ProductID uint      `gorm:"column:product_id"`
    Amount    float64   `gorm:"column:amount"`
    Status    string    `gorm:"column:status"`
    CreatedAt time.Time `gorm:"column:created_at"`
}

func (ShardedOrder) TableName() string {
    return "orders"
}

// 分片数据库操作
type ShardedDB struct {
    manager *ShardingManager
}

func NewShardedDB(manager *ShardingManager) *ShardedDB {
    return &ShardedDB{manager: manager}
}

// 插入数据
func (sdb *ShardedDB) CreateUser(user *ShardedUser) error {
    dbName := sdb.manager.strategy.GetDatabaseName(user.TableName(), user.UserID)
    tableName := sdb.manager.strategy.GetTableName(user.TableName(), user.UserID)
    
    db, exists := sdb.manager.dbMap[dbName]
    if !exists {
        return fmt.Errorf("数据库 %s 不存在", dbName)
    }
    
    // 动态设置表名
    return db.Table(tableName).Create(user).Error
}

// 查询数据
func (sdb *ShardedDB) GetUser(userID uint) (*ShardedUser, error) {
    dbName := sdb.manager.strategy.GetDatabaseName("users", userID)
    tableName := sdb.manager.strategy.GetTableName("users", userID)
    
    db, exists := sdb.manager.dbMap[dbName]
    if !exists {
        return nil, fmt.Errorf("数据库 %s 不存在", dbName)
    }
    
    var user ShardedUser
    err := db.Table(tableName).Where("user_id = ?", userID).First(&user).Error
    if err != nil {
        return nil, err
    }
    
    return &user, nil
}

// 跨分片查询
func (sdb *ShardedDB) GetUsersByCondition(condition map[string]interface{}) ([]ShardedUser, error) {
    var allUsers []ShardedUser
    
    // 遍历所有数据库
    for dbName, db := range sdb.manager.dbMap {
        fmt.Printf("查询数据库: %s\n", dbName)
        
        // 遍历可能的表
        for i := 0; i < 10; i++ { // 假设有10个分表
            tableName := fmt.Sprintf("users_%d", i)
            var users []ShardedUser
            
            query := db.Table(tableName)
            for key, value := range condition {
                query = query.Where(fmt.Sprintf("%s = ?", key), value)
            }
            
            err := query.Find(&users).Error
            if err != nil {
                fmt.Printf("查询表 %s 失败: %v\n", tableName, err)
                continue
            }
            
            allUsers = append(allUsers, users...)
        }
    }
    
    return allUsers, nil
}

// 批量插入优化
func (sdb *ShardedDB) BatchCreateUsers(users []*ShardedUser) error {
    // 按分片键分组
    shardedUsers := make(map[string][]*ShardedUser)
    
    for _, user := range users {
        dbName := sdb.manager.strategy.GetDatabaseName(user.TableName(), user.UserID)
        tableName := sdb.manager.strategy.GetTableName(user.TableName(), user.UserID)
        key := fmt.Sprintf("%s.%s", dbName, tableName)
        shardedUsers[key] = append(shardedUsers[key], user)
    }
    
    // 批量插入到各个分片
    for key, userGroup := range shardedUsers {
        parts := strings.Split(key, ".")
        dbName, tableName := parts[0], parts[1]
        
        db, exists := sdb.manager.dbMap[dbName]
        if !exists {
            fmt.Printf("数据库 %s 不存在，跳过插入\n", dbName)
            continue
        }
        
        // 批量插入
        if err := db.Table(tableName).CreateInBatches(userGroup, 1000).Error; err != nil {
            fmt.Printf("批量插入到 %s 失败: %v\n", key, err)
        } else {
            fmt.Printf("成功插入 %d 条记录到 %s\n", len(userGroup), key)
        }
    }
    
    return nil
}

// 分片统计
func (sdb *ShardedDB) GetShardingStats() map[string]interface{} {
    stats := make(map[string]interface{})
    
    totalRecords := 0
    dbStats := make(map[string]int)
    
    // 统计各个分片的数据量
    for dbName, db := range sdb.manager.dbMap {
        dbTotal := 0
        
        for i := 0; i < 10; i++ { // 假设有10个分表
            tableName := fmt.Sprintf("users_%d", i)
            var count int64
            db.Table(tableName).Count(&count)
            dbTotal += int(count)
        }
        
        dbStats[dbName] = dbTotal
        totalRecords += dbTotal
    }
    
    stats["total_records"] = totalRecords
    stats["database_stats"] = dbStats
    
    return stats
}

// 分片迁移工具
type ShardingMigrator struct {
    manager *ShardingManager
}

func NewShardingMigrator(manager *ShardingManager) *ShardingMigrator {
    return &ShardingMigrator{manager: manager}
}

// 创建分片表结构
func (sm *ShardingMigrator) CreateShardedTables() error {
    fmt.Println("创建分片表结构...")
    
    // 在每个数据库中创建分表
    for dbName, db := range sm.manager.dbMap {
        fmt.Printf("在数据库 %s 中创建分表...\n", dbName)
        
        // 创建10个分表
        for i := 0; i < 10; i++ {
            tableName := fmt.Sprintf("users_%d", i)
            if err := db.Table(tableName).AutoMigrate(&ShardedUser{}); err != nil {
                fmt.Printf("创建表 %s 失败: %v\n", tableName, err)
            } else {
                fmt.Printf("成功创建表 %s\n", tableName)
            }
        }
    }
    
    return nil
}

// 数据重分片
func (sm *ShardingMigrator) ReshardData(oldStrategy, newStrategy ShardingStrategy) error {
    fmt.Println("开始数据重分片...")
    
    // 这是一个复杂的过程，需要：
    // 1. 停止写入操作
    // 2. 逐个迁移数据
    // 3. 验证数据完整性
    // 4. 切换到新分片策略
    // 5. 清理旧数据
    
    fmt.Println("数据重分片完成")
    return nil
}
```

### 读写分离

#### 读写分离实现
```go
package main

import (
    "context"
    "database/sql"
    "fmt"
    "math/rand"
    "sync"
    "time"
    
    "gorm.io/gorm"
)

// 数据库角色定义
type DBRole string

const (
    Master DBRole = "master"
    Slave  DBRole = "slave"
)

// 读写分离数据库连接池
type ReadWriteSplitDB struct {
    master   *gorm.DB
    slaves   []*gorm.DB
    mutex    sync.RWMutex
    strategy LoadBalanceStrategy
}

// 负载均衡策略接口
type LoadBalanceStrategy interface {
    SelectSlave(slaves []*gorm.DB) *gorm.DB
}

// 轮询负载均衡
type RoundRobinStrategy struct {
    currentIndex int
    mutex        sync.Mutex
}

func (rrs *RoundRobinStrategy) SelectSlave(slaves []*gorm.DB) *gorm.DB {
    rrs.mutex.Lock()
    defer rrs.mutex.Unlock()
    
    if len(slaves) == 0 {
        return nil
    }
    
    slave := slaves[rrs.currentIndex%len(slaves)]
    rrs.currentIndex++
    return slave
}

// 随机负载均衡
type RandomStrategy struct{}

func (rs *RandomStrategy) SelectSlave(slaves []*gorm.DB) *gorm.DB {
    if len(slaves) == 0 {
        return nil
    }
    
    return slaves[rand.Intn(len(slaves))]
}

// 权重负载均衡
type WeightedStrategy struct {
    weights []int
    mutex   sync.Mutex
}

func NewWeightedStrategy(weights []int) *WeightedStrategy {
    return &WeightedStrategy{weights: weights}
}

func (ws *WeightedStrategy) SelectSlave(slaves []*gorm.DB) *gorm.DB {
    ws.mutex.Lock()
    defer ws.mutex.Unlock()
    
    if len(slaves) == 0 || len(ws.weights) != len(slaves) {
        return nil
    }
    
    // 根据权重选择从库
    totalWeight := 0
    for _, weight := range ws.weights {
        totalWeight += weight
    }
    
    randWeight := rand.Intn(totalWeight)
    currentWeight := 0
    
    for i, weight := range ws.weights {
        currentWeight += weight
        if randWeight < currentWeight {
            return slaves[i]
        }
    }
    
    return slaves[0]
}

// 创建读写分离数据库
func NewReadWriteSplitDB(master *gorm.DB, slaves []*gorm.DB, strategy LoadBalanceStrategy) *ReadWriteSplitDB {
    return &ReadWriteSplitDB{
        master:   master,
        slaves:   slaves,
        strategy: strategy,
    }
}

// 获取主库连接（用于写操作）
func (rws *ReadWriteSplitDB) Master() *gorm.DB {
    return rws.master
}

// 获取从库连接（用于读操作）
func (rws *ReadWriteSplitDB) Slave() *gorm.DB {
    rws.mutex.RLock()
    defer rws.mutex.RUnlock()
    
    return rws.strategy.SelectSlave(rws.slaves)
}

// 读写分离中间件
type ReadWriteSplitMiddleware struct {
    db *ReadWriteSplitDB
}

func NewReadWriteSplitMiddleware(db *ReadWriteSplitDB) *ReadWriteSplitMiddleware {
    return &ReadWriteSplitMiddleware{db: db}
}

// 写操作
func (rwm *ReadWriteSplitMiddleware) WriteOperation(operation func(*gorm.DB) error) error {
    return operation(rwm.db.Master())
}

// 读操作
func (rwm *ReadWriteSplitMiddleware) ReadOperation(operation func(*gorm.DB) error) error {
    return operation(rwm.db.Slave())
}

// 自动读写分离
type AutoReadWriteSplit struct {
    db *ReadWriteSplitDB
}

func NewAutoReadWriteSplit(db *ReadWriteSplitDB) *AutoReadWriteSplit {
    return &AutoReadWriteSplit{db: db}
}

// 根据SQL类型自动选择数据库
func (ars *AutoReadWriteSplit) ExecuteSQL(sql string, operation func(*gorm.DB) error) error {
    if ars.isWriteOperation(sql) {
        return operation(ars.db.Master())
    } else {
        return operation(ars.db.Slave())
    }
}

func (ars *AutoReadWriteSplit) isWriteOperation(sql string) bool {
    sql = strings.ToUpper(strings.TrimSpace(sql))
    writeKeywords := []string{"INSERT", "UPDATE", "DELETE", "CREATE", "ALTER", "DROP", "TRUNCATE"}
    
    for _, keyword := range writeKeywords {
        if strings.HasPrefix(sql, keyword) {
            return true
        }
    }
    
    return false
}

// 读写分离实体示例
type RWUser struct {
    ID        uint      `gorm:"primaryKey"`
    Name      string    `gorm:"column:name"`
    Email     string    `gorm:"column:email"`
    CreatedAt time.Time `gorm:"column:created_at"`
}

func (RWUser) TableName() string {
    return "users"
}

// 读写分离服务
type ReadWriteSplitService struct {
    db *AutoReadWriteSplit
}

func NewReadWriteSplitService(db *AutoReadWriteSplit) *ReadWriteSplitService {
    return &ReadWriteSplitService{db: db}
}

// 创建用户（写操作）
func (rwss *ReadWriteSplitService) CreateUser(user *RWUser) error {
    return rwss.db.ExecuteSQL("INSERT INTO users", func(gdb *gorm.DB) error {
        return gdb.Create(user).Error
    })
}

// 查询用户（读操作）
func (rwss *ReadWriteSplitService) GetUser(id uint) (*RWUser, error) {
    var user RWUser
    err := rwss.db.ExecuteSQL("SELECT * FROM users", func(gdb *gorm.DB) error {
        return gdb.First(&user, id).Error
    })
    
    if err != nil {
        return nil, err
    }
    return &user, nil
}

// 更新用户（写操作）
func (rwss *ReadWriteSplitService) UpdateUser(user *RWUser) error {
    return rwss.db.ExecuteSQL("UPDATE users", func(gdb *gorm.DB) error {
        return gdb.Save(user).Error
    })
}

// 批量查询用户（读操作）
func (rwss *ReadWriteSplitService) ListUsers() ([]RWUser, error) {
    var users []RWUser
    err := rwss.db.ExecuteSQL("SELECT * FROM users", func(gdb *gorm.DB) error {
        return gdb.Find(&users).Error
    })
    
    return users, err
}

// 读写分离监控
type ReadWriteSplitMonitor struct {
    masterStats DBStats
    slaveStats  []DBStats
    mutex       sync.RWMutex
}

type DBStats struct {
    Queries     int64
    LastQuery   time.Time
    AvgResponse time.Duration
    Errors      int64
}

func NewReadWriteSplitMonitor() *ReadWriteSplitMonitor {
    return &ReadWriteSplitMonitor{
        slaveStats: make([]DBStats, 0),
    }
}

// 监控主库状态
func (rwsm *ReadWriteSplitMonitor) MonitorMasterStats(db *gorm.DB) {
    // 实现主库监控逻辑
    fmt.Println("监控主库状态...")
}

// 监控从库状态
func (rwsm *ReadWriteSplitMonitor) MonitorSlaveStats(dbs []*gorm.DB) {
    // 实现从库监控逻辑
    fmt.Println("监控从库状态...")
}

// 健康检查
func (rwsm *ReadWriteSplitMonitor) HealthCheck(master *gorm.DB, slaves []*gorm.DB) error {
    // 检查主库连接
    if err := master.Exec("SELECT 1").Error; err != nil {
        return fmt.Errorf("主库连接失败: %v", err)
    }
    
    // 检查从库连接
    for i, slave := range slaves {
        if err := slave.Exec("SELECT 1").Error; err != nil {
            return fmt.Errorf("从库 %d 连接失败: %v", i, err)
        }
    }
    
    fmt.Println("读写分离数据库健康检查通过")
    return nil
}

// 故障转移
type FailoverManager struct {
    master   *gorm.DB
    slaves   []*gorm.DB
    downSlaves []int
    mutex    sync.RWMutex
}

func NewFailoverManager(master *gorm.DB, slaves []*gorm.DB) *FailoverManager {
    return &FailoverManager{
        master: master,
        slaves: slaves,
    }
}

// 检测从库故障
func (fm *FailoverManager) DetectSlaveFailure() {
    fm.mutex.Lock()
    defer fm.mutex.Unlock()
    
    for i, slave := range fm.slaves {
        if err := slave.Exec("SELECT 1").Error; err != nil {
            fmt.Printf("检测到从库 %d 故障: %v\n", i, err)
            fm.downSlaves = append(fm.downSlaves, i)
        }
    }
}

// 从库恢复
func (fm *FailoverManager) RecoverSlave(slaveIndex int) {
    fm.mutex.Lock()
    defer fm.mutex.Unlock()
    
    // 从故障列表中移除
    for i, downIndex := range fm.downSlaves {
        if downIndex == slaveIndex {
            fm.downSlaves = append(fm.downSlaves[:i], fm.downSlaves[i+1:]...)
            fmt.Printf("从库 %d 恢复\n", slaveIndex)
            break
        }
    }
}

// 动态调整读写分离配置
type DynamicReadWriteSplit struct {
    baseConfig *ReadWriteSplitDB
    currentConfig *ReadWriteSplitDB
    mutex sync.RWMutex
}

func NewDynamicReadWriteSplit(baseConfig *ReadWriteSplitDB) *DynamicReadWriteSplit {
    return &DynamicReadWriteSplit{
        baseConfig: baseConfig,
        currentConfig: baseConfig,
    }
}

// 根据负载动态调整
func (drws *DynamicReadWriteSplit) AdjustByLoad() {
    drws.mutex.Lock()
    defer drws.mutex.Unlock()
    
    // 根据当前负载情况调整配置
    // 例如：增加从库、调整权重等
    fmt.Println("根据负载动态调整读写分离配置...")
}
```

### 数据库监控

#### 数据库监控系统
```go
package main

import (
    "context"
    "database/sql"
    "encoding/json"
    "fmt"
    "log"
    "net/http"
    "sync"
    "sync/atomic"
    "time"
    
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promhttp"
    "gorm.io/gorm"
)

// 数据库监控指标
type DatabaseMetrics struct {
    // 查询相关指标
    QueryCount        *prometheus.CounterVec
    QueryDuration     *prometheus.HistogramVec
    QueryErrors       *prometheus.CounterVec
    SlowQueries       *prometheus.CounterVec
    
    // 连接池相关指标
    ConnectionCount   *prometheus.GaugeVec
    ConnectionUsage   *prometheus.GaugeVec
    ConnectionWait    *prometheus.CounterVec
    
    // 性能指标
    TransactionCount  *prometheus.CounterVec
    TransactionErrors *prometheus.CounterVec
    LockWaits         *prometheus.CounterVec
}

// 创建数据库监控指标
func NewDatabaseMetrics() *DatabaseMetrics {
    return &DatabaseMetrics{
        QueryCount: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "db_query_total",
                Help: "Total number of database queries",
            },
            []string{"db", "table", "operation"},
        ),
        QueryDuration: prometheus.NewHistogramVec(
            prometheus.HistogramOpts{
                Name:    "db_query_duration_seconds",
                Help:    "Database query duration in seconds",
                Buckets: prometheus.ExponentialBuckets(0.001, 2, 15),
            },
            []string{"db", "table", "operation"},
        ),
        QueryErrors: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "db_query_errors_total",
                Help: "Total number of database query errors",
            },
            []string{"db", "table", "operation", "error"},
        ),
        SlowQueries: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "db_slow_queries_total",
                Help: "Total number of slow database queries",
            },
            []string{"db", "table", "operation", "threshold"},
        ),
        ConnectionCount: prometheus.NewGaugeVec(
            prometheus.GaugeOpts{
                Name: "db_connections_total",
                Help: "Total number of database connections",
            },
            []string{"db", "type"},
        ),
        ConnectionUsage: prometheus.NewGaugeVec(
            prometheus.GaugeOpts{
                Name: "db_connection_usage_ratio",
                Help: "Database connection usage ratio",
            },
            []string{"db"},
        ),
        ConnectionWait: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "db_connection_waits_total",
                Help: "Total number of connection waits",
            },
            []string{"db"},
        ),
        TransactionCount: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "db_transactions_total",
                Help: "Total number of database transactions",
            },
            []string{"db", "status"},
        ),
        TransactionErrors: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "db_transaction_errors_total",
                Help: "Total number of database transaction errors",
            },
            []string{"db", "error"},
        ),
        LockWaits: prometheus.NewCounterVec(
            prometheus.CounterOpts{
                Name: "db_lock_waits_total",
                Help: "Total number of database lock waits",
            },
            []string{"db", "table"},
        ),
    }
}

// 注册监控指标
func (dm *DatabaseMetrics) Register() {
    prometheus.MustRegister(
        dm.QueryCount,
        dm.QueryDuration,
        dm.QueryErrors,
        dm.SlowQueries,
        dm.ConnectionCount,
        dm.ConnectionUsage,
        dm.ConnectionWait,
        dm.TransactionCount,
        dm.TransactionErrors,
        dm.LockWaits,
    )
}

// 数据库监控器
type DatabaseMonitor struct {
    db      *sql.DB
    metrics *DatabaseMetrics
    config  MonitorConfig
    stopCh  chan struct{}
}

type MonitorConfig struct {
    SlowQueryThreshold time.Duration
    CollectInterval    time.Duration
    EnableSlowQueryLog bool
}

func NewDatabaseMonitor(db *sql.DB, metrics *DatabaseMetrics, config MonitorConfig) *DatabaseMonitor {
    return &DatabaseMonitor{
        db:      db,
        metrics: metrics,
        config:  config,
        stopCh:  make(chan struct{}),
    }
}

// 启动监控
func (dm *DatabaseMonitor) Start() {
    go dm.collectMetrics()
    go dm.collectConnectionStats()
    go dm.collectPerformanceStats()
}

// 停止监控
func (dm *DatabaseMonitor) Stop() {
    close(dm.stopCh)
}

// 收集基本指标
func (dm *DatabaseMonitor) collectMetrics() {
    ticker := time.NewTicker(dm.config.CollectInterval)
    defer ticker.Stop()
    
    for {
        select {
        case <-ticker.C:
            dm.collectConnectionStats()
            dm.collectPerformanceStats()
        case <-dm.stopCh:
            return
        }
    }
}

// 收集连接池统计信息
func (dm *DatabaseMonitor) collectConnectionStats() {
    stats := dm.db.Stats()
    
    // 连接数统计
    dm.metrics.ConnectionCount.WithLabelValues("default", "max_open").Set(float64(stats.MaxOpenConnections))
    dm.metrics.ConnectionCount.WithLabelValues("default", "open").Set(float64(stats.OpenConnections))
    dm.metrics.ConnectionCount.WithLabelValues("default", "in_use").Set(float64(stats.InUse))
    dm.metrics.ConnectionCount.WithLabelValues("default", "idle").Set(float64(stats.Idle))
    
    // 连接使用率
    if stats.MaxOpenConnections > 0 {
        usageRatio := float64(stats.InUse) / float64(stats.MaxOpenConnections)
        dm.metrics.ConnectionUsage.WithLabelValues("default").Set(usageRatio)
    }
    
    // 等待统计
    dm.metrics.ConnectionWait.WithLabelValues("default").Add(float64(stats.WaitCount))
}

// 收集性能统计信息
func (dm *DatabaseMonitor) collectPerformanceStats() {
    // 收集慢查询信息
    dm.collectSlowQueries()
    
    // 收集锁等待信息
    dm.collectLockWaits()
}

func (dm *DatabaseMonitor) collectSlowQueries() {
    // 查询慢查询日志
    var slowQueries []struct {
        DigestText    string
        CountStar     int64
        AvgTimerWait  int64
        MaxTimerWait  int64
    }
    
    err := dm.db.QueryRow(`
        SELECT 
            DIGEST_TEXT,
            COUNT_STAR,
            AVG_TIMER_WAIT,
            MAX_TIMER_WAIT
        FROM performance_schema.events_statements_summary_by_digest 
        WHERE AVG_TIMER_WAIT > ? 
        ORDER BY AVG_TIMER_WAIT DESC
        LIMIT 10
    `, dm.config.SlowQueryThreshold.Nanoseconds()).Scan(&slowQueries)
    
    if err != nil && err != sql.ErrNoRows {
        log.Printf("收集慢查询信息失败: %v", err)
        return
    }
    
    for _, query := range slowQueries {
        threshold := fmt.Sprintf("%.3fs", float64(dm.config.SlowQueryThreshold)/float64(time.Second))
        dm.metrics.SlowQueries.WithLabelValues("default", "unknown", "query", threshold).Add(float64(query.CountStar))
    }
}

func (dm *DatabaseMonitor) collectLockWaits() {
    // 收集锁等待信息
    var lockWaits int64
    err := dm.db.QueryRow(`
        SELECT COUNT(*) 
        FROM performance_schema.metadata_locks 
        WHERE OWNER_THREAD_ID IS NOT NULL
    `).Scan(&lockWaits)
    
    if err != nil && err != sql.ErrNoRows {
        log.Printf("收集锁等待信息失败: %v", err)
        return
    }
    
    dm.metrics.LockWaits.WithLabelValues("default", "all").Add(float64(lockWaits))
}

// 查询监控装饰器
type QueryMonitor struct {
    db      *gorm.DB
    metrics *DatabaseMetrics
    dbName  string
}

func NewQueryMonitor(db *gorm.DB, metrics *DatabaseMetrics, dbName string) *QueryMonitor {
    return &QueryMonitor{
        db:      db,
        metrics: metrics,
        dbName:  dbName,
    }
}

// 监控查询执行
func (qm *QueryMonitor) MonitorQuery(table, operation string, queryFunc func() error) error {
    startTime := time.Now()
    
    // 执行查询
    err := queryFunc()
    
    duration := time.Since(startTime).Seconds()
    
    // 记录查询次数
    qm.metrics.QueryCount.WithLabelValues(qm.dbName, table, operation).Inc()
    
    // 记录查询耗时
    qm.metrics.QueryDuration.WithLabelValues(qm.dbName, table, operation).Observe(duration)
    
    // 记录错误
    if err != nil {
        qm.metrics.QueryErrors.WithLabelValues(qm.dbName, table, operation, err.Error()).Inc()
    }
    
    // 记录慢查询
    if duration > 1.0 { // 超过1秒认为是慢查询
        qm.metrics.SlowQueries.WithLabelValues(qm.dbName, table, operation, "1s").Inc()
    }
    
    return err
}

// 事务监控
type TransactionMonitor struct {
    metrics *DatabaseMetrics
    dbName  string
}

func NewTransactionMonitor(metrics *DatabaseMetrics, dbName string) *TransactionMonitor {
    return &TransactionMonitor{
        metrics: metrics,
        dbName:  dbName,
    }
}

func (tm *TransactionMonitor) MonitorTransaction(txFunc func() error) error {
    startTime := time.Now()
    
    // 执行事务
    err := txFunc()
    
    duration := time.Since(startTime)
    
    // 记录事务次数
    if err != nil {
        tm.metrics.TransactionCount.WithLabelValues(tm.dbName, "failed").Inc()
        tm.metrics.TransactionErrors.WithLabelValues(tm.dbName, err.Error()).Inc()
    } else {
        tm.metrics.TransactionCount.WithLabelValues(tm.dbName, "success").Inc()
    }
    
    return err
}

// HTTP监控接口
type MonitorAPI struct {
    monitor *DatabaseMonitor
    metrics *DatabaseMetrics
}

func NewMonitorAPI(monitor *DatabaseMonitor, metrics *DatabaseMetrics) *MonitorAPI {
    return &MonitorAPI{
        monitor: monitor,
        metrics: metrics,
    }
}

func (ma *MonitorAPI) StartHTTPServer(port string) {
    http.Handle("/metrics", promhttp.Handler())
    http.HandleFunc("/health", ma.healthHandler)
    http.HandleFunc("/stats", ma.statsHandler)
    
    log.Printf("监控API启动在端口 %s", port)
    log.Fatal(http.ListenAndServe(":"+port, nil))
}

func (ma *MonitorAPI) healthHandler(w http.ResponseWriter, r *http.Request) {
    w.Header().Set("Content-Type", "application/json")
    
    health := map[string]interface{}{
        "status": "healthy",
        "timestamp": time.Now(),
    }
    
    json.NewEncoder(w).Encode(health)
}

func (ma *MonitorAPI) statsHandler(w http.ResponseWriter, r *http.Request) {
    w.Header().Set("Content-Type", "application/json")
    
    stats := dm.db.Stats()
    response := map[string]interface{}{
        "connection_stats": map[string]interface{}{
            "max_open_connections": stats.MaxOpenConnections,
            "open_connections":     stats.OpenConnections,
            "in_use":              stats.InUse,
            "idle":                stats.Idle,
            "wait_count":          stats.WaitCount,
            "wait_duration":       stats.WaitDuration.String(),
        },
        "timestamp": time.Now(),
    }
    
    json.NewEncoder(w).Encode(response)
}

// 告警系统
type AlertManager struct {
    rules   []AlertRule
    alerts  chan Alert
    stopCh  chan struct{}
}

type AlertRule struct {
    Name        string
    Condition   func() bool
    Threshold   float64
    Duration    time.Duration
    Severity    string
    Description string
}

type Alert struct {
    Rule      AlertRule
    Triggered time.Time
    Value     float64
}

func NewAlertManager() *AlertManager {
    return &AlertManager{
        alerts: make(chan Alert, 100),
        stopCh: make(chan struct{}),
    }
}

func (am *AlertManager) AddRule(rule AlertRule) {
    am.rules = append(am.rules, rule)
}

func (am *AlertManager) Start() {
    go am.checkRules()
    go am.processAlerts()
}

func (am *AlertManager) Stop() {
    close(am.stopCh)
}

func (am *AlertManager) checkRules() {
    ticker := time.NewTicker(time.Minute)
    defer ticker.Stop()
    
    for {
        select {
        case <-ticker.C:
            am.evaluateRules()
        case <-am.stopCh:
            return
        }
    }
}

func (am *AlertManager) evaluateRules() {
    for _, rule := range am.rules {
        if rule.Condition() {
            alert := Alert{
                Rule:      rule,
                Triggered: time.Now(),
                Value:     rule.Threshold,
            }
            select {
            case am.alerts <- alert:
            default:
                log.Println("告警队列已满，丢弃告警")
            }
        }
    }
}

func (am *AlertManager) processAlerts() {
    for {
        select {
        case alert := <-am.alerts:
            am.sendAlert(alert)
        case <-am.stopCh:
            return
        }
    }
}

func (am *AlertManager) sendAlert(alert Alert) {
    // 发送告警通知（邮件、短信、微信等）
    log.Printf("触发告警: %s, 严重程度: %s, 描述: %s",
        alert.Rule.Name, alert.Rule.Severity, alert.Rule.Description)
}

// 性能分析工具
type PerformanceAnalyzer struct {
    db *sql.DB
}

func NewPerformanceAnalyzer(db *sql.DB) *PerformanceAnalyzer {
    return &PerformanceAnalyzer{db: db}
}

// 分析查询性能
func (pa *PerformanceAnalyzer) AnalyzeQueryPerformance() {
    fmt.Println("分析查询性能...")
    
    // 分析慢查询
    pa.analyzeSlowQueries()
    
    // 分析索引使用
    pa.analyzeIndexUsage()
    
    // 分析锁等待
    pa.analyzeLockWaits()
}

func (pa *PerformanceAnalyzer) analyzeSlowQueries() {
    fmt.Println("分析慢查询...")
    
    rows, err := pa.db.Query(`
        SELECT 
            DIGEST_TEXT,
            COUNT_STAR,
            AVG_TIMER_WAIT/1000000000 as AVG_TIME_MS,
            MAX_TIMER_WAIT/1000000000 as MAX_TIME_MS,
            FIRST_SEEN,
            LAST_SEEN
        FROM performance_schema.events_statements_summary_by_digest 
        WHERE AVG_TIMER_WAIT > 1000000000000  -- 平均执行时间超过1秒
        ORDER BY AVG_TIMER_WAIT DESC
        LIMIT 20
    `)
    if err != nil {
        log.Printf("查询慢查询统计失败: %v", err)
        return
    }
    defer rows.Close()
    
    fmt.Println("慢查询分析结果:")
    for rows.Next() {
        var digestText string
        var countStar, avgTimeMs, maxTimeMs int64
        var firstSeen, lastSeen string
        
        err := rows.Scan(&digestText, &countStar, &avgTimeMs, &maxTimeMs, &firstSeen, &lastSeen)
        if err != nil {
            log.Printf("扫描慢查询结果失败: %v", err)
            continue
        }
        
        fmt.Printf("  查询: %s\n", digestText)
        fmt.Printf("    执行次数: %d, 平均时间: %d ms, 最大时间: %d ms\n", 
            countStar, avgTimeMs, maxTimeMs)
        fmt.Printf("    首次出现: %s, 最后出现: %s\n\n", firstSeen, lastSeen)
    }
}

func (pa *PerformanceAnalyzer) analyzeIndexUsage() {
    fmt.Println("分析索引使用情况...")
    
    rows, err := pa.db.Query(`
        SELECT 
            TABLE_NAME,
            INDEX_NAME,
            CARDINALITY,
            SUB_PART,
            PACKED,
            NULLABLE,
            INDEX_TYPE,
            COMMENT
        FROM information_schema.STATISTICS 
        WHERE TABLE_SCHEMA = DATABASE()
        ORDER BY TABLE_NAME, SEQ_IN_INDEX
    `)
    if err != nil {
        log.Printf("查询索引统计失败: %v", err)
        return
    }
    defer rows.Close()
    
    fmt.Println("索引使用分析:")
    for rows.Next() {
        var tableName, indexName, indexType, comment string
        var cardinality, subPart sql.NullInt64
        var packed, nullable sql.NullString
        
        err := rows.Scan(&tableName, &indexName, &cardinality, &subPart, &packed, &nullable, &indexType, &comment)
        if err != nil {
            log.Printf("扫描索引结果失败: %v", err)
            continue
        }
        
        fmt.Printf("  表: %s, 索引: %s, 类型: %s\n", tableName, indexName, indexType)
        if cardinality.Valid {
            fmt.Printf("    基数: %d", cardinality.Int64)
        }
        if comment != "" {
            fmt.Printf(", 备注: %s", comment)
        }
        fmt.Println()
    }
}

func (pa *PerformanceAnalyzer) analyzeLockWaits() {
    fmt.Println("分析锁等待情况...")
    
    rows, err := pa.db.Query(`
        SELECT 
            OBJECT_SCHEMA,
            OBJECT_NAME,
            LOCK_TYPE,
            LOCK_DURATION,
            LOCK_STATUS,
            OWNER_THREAD_ID,
            OWNER_EVENT_ID
        FROM performance_schema.metadata_locks 
        WHERE LOCK_STATUS = 'PENDING'
        LIMIT 10
    `)
    if err != nil {
        log.Printf("查询锁等待信息失败: %v", err)
        return
    }
    defer rows.Close()
    
    fmt.Println("锁等待分析:")
    if !rows.Next() {
        fmt.Println("  当前无锁等待")
        return
    }
    
    for rows.Next() {
        var schema, name, lockType, duration, status string
        var threadID, eventID int64
        
        err := rows.Scan(&schema, &name, &lockType, &duration, &status, &threadID, &eventID)
        if err != nil {
            log.Printf("扫描锁等待结果失败: %v", err)
            continue
        }
        
        fmt.Printf("  库名: %s, 表名: %s, 锁类型: %s, 状态: %s\n", 
            schema, name, lockType, status)
        fmt.Printf("    线程ID: %d, 事件ID: %d\n", threadID, eventID)
    }
}

// 数据库优化建议生成器
type OptimizationAdvisor struct {
    db *sql.DB
}

func NewOptimizationAdvisor(db *sql.DB) *OptimizationAdvisor {
    return &OptimizationAdvisor{db: db}
}

// 生成优化建议
func (oa *OptimizationAdvisor) GenerateRecommendations() []string {
    var recommendations []string
    
    // 检查慢查询
    slowQueryRecs := oa.checkSlowQueries()
    recommendations = append(recommendations, slowQueryRecs...)
    
    // 检查索引
    indexRecs := oa.checkIndexes()
    recommendations = append(recommendations, indexRecs...)
    
    // 检查连接池
    connectionRecs := oa.checkConnectionPool()
    recommendations = append(recommendations, connectionRecs...)
    
    // 检查配置
    configRecs := oa.checkConfiguration()
    recommendations = append(recommendations, configRecs...)
    
    return recommendations
}

func (oa *OptimizationAdvisor) checkSlowQueries() []string {
    var recommendations []string
    
    // 检查是否有慢查询
    var slowQueryCount int
    err := oa.db.QueryRow(`
        SELECT COUNT(*) 
        FROM performance_schema.events_statements_summary_by_digest 
        WHERE AVG_TIMER_WAIT > 1000000000000
    `).Scan(&slowQueryCount)
    
    if err != nil {
        log.Printf("检查慢查询失败: %v", err)
        return recommendations
    }
    
    if slowQueryCount > 0 {
        recommendations = append(recommendations, 
            "发现慢查询，建议：",
            "1. 为WHERE子句中的列添加索引",
            "2. 优化JOIN查询",
            "3. 避免SELECT *，只选择需要的字段",
            "4. 使用LIMIT限制结果集大小")
    }
    
    return recommendations
}

func (oa *OptimizationAdvisor) checkIndexes() []string {
    var recommendations []string
    
    // 检查未使用的索引
    rows, err := oa.db.Query(`
        SELECT 
            OBJECT_SCHEMA,
            OBJECT_NAME,
            INDEX_NAME
        FROM performance_schema.table_io_waits_summary_by_index_usage 
        WHERE INDEX_NAME IS NOT NULL 
        AND COUNT_STAR = 0
        LIMIT 5
    `)
    if err != nil {
        log.Printf("检查未使用索引失败: %v", err)
        return recommendations
    }
    defer rows.Close()
    
    hasUnusedIndexes := false
    for rows.Next() {
        hasUnusedIndexes = true
        break
    }
    
    if hasUnusedIndexes {
        recommendations = append(recommendations, 
            "发现未使用的索引，建议删除以提高写入性能")
    }
    
    return recommendations
}

func (oa *OptimizationAdvisor) checkConnectionPool() []string {
    var recommendations []string
    
    stats := oa.db.Stats()
    usageRatio := float64(stats.InUse) / float64(stats.MaxOpenConnections)
    
    if usageRatio > 0.8 {
        recommendations = append(recommendations, 
            fmt.Sprintf("连接池使用率过高(%.2f%%)，建议增加最大连接数", usageRatio*100))
    }
    
    if stats.WaitCount > 0 {
        recommendations = append(recommendations, 
            "有连接等待，建议优化连接池配置或增加数据库连接数")
    }
    
    return recommendations
}

func (oa *OptimizationAdvisor) checkConfiguration() []string {
    var recommendations []string
    
    // 检查一些重要的MySQL配置参数
    configChecks := []struct {
        variable string
        expected string
        desc     string
    }{
        {"innodb_buffer_pool_size", "", "InnoDB缓冲池大小"},
        {"max_connections", "", "最大连接数"},
        {"query_cache_size", "", "查询缓存大小"},
        {"tmp_table_size", "", "临时表大小"},
    }
    
    for _, check := range configChecks {
        var value string
        err := oa.db.QueryRow("SHOW VARIABLES LIKE ?", check.variable).Scan(&check.variable, &value)
        if err != nil {
            continue
        }
        
        // 这里可以添加更详细的配置检查逻辑
        recommendations = append(recommendations, 
            fmt.Sprintf("检查%s配置: 当前值 %s", check.desc, value))
    }
    
    return recommendations
}
```