# ç›®å½•

- [ä¸€,å…¨å±€ç¯å¢ƒå‚æ•°å®šä¹‰](#ä¸€å…¨å±€ç¯å¢ƒå‚æ•°å®šä¹‰)
- [äºŒ,å·¥å…·å‡½æ•°å’Œå¥–åŠ±å‡½æ•°å®šä¹‰](#äºŒå·¥å…·å‡½æ•°å’Œå¥–åŠ±å‡½æ•°å®šä¹‰)
- [ä¸‰,è¿­ä»£æ–¹æ³•](#ä¸‰è¿­ä»£æ–¹æ³•)
  - [1.è’™ç‰¹å¡æ´›æ–¹æ³•](#1è’™ç‰¹å¡æ´›æ–¹æ³•)
    - [1.1 ç­–ç•¥ç”Ÿæˆ](#11-ç­–ç•¥ç”Ÿæˆ)
    - [1.2 On-policy æ–¹æ³•](#12-on-policy-æ–¹æ³•)
    - [1.3 Off-policy æ–¹æ³•](#13-off-policy-æ–¹æ³•)
  - [2.åŠ¨æ€è§„åˆ’æ–¹æ³•](#2åŠ¨æ€è§„åˆ’æ–¹æ³•)
    - [2.1  ç­–ç•¥è¯„ä¼°](#21--ç­–ç•¥è¯„ä¼°)
    - [2.2 ä»·å€¼è¿­ä»£](#22-ä»·å€¼è¿­ä»£)
  - [3.æ—¶åˆ†åºåˆ—æ–¹æ³•](#3æ—¶åˆ†åºåˆ—æ–¹æ³•)
    - [3.1 Q-learning](#31-q-learning)
    - [3.2 SARSA](#32-sarsa)
    - [3.3 N-Sarsa](#33-n-sarsa)
- [å››,è¾“å‡ºç­–ç•¥å’Œä»·å€¼](#å››è¾“å‡ºç­–ç•¥å’Œä»·å€¼)
- [äº”,è°ƒè¯•](#äº”è°ƒè¯•)
- [å…­,ç½‘ç»œæ¨¡å‹](#å…­ç½‘ç»œæ¨¡å‹)

# ä¸€,å…¨å±€ç¯å¢ƒå‚æ•°å®šä¹‰


```python
import random

import numpy as np
from collections import defaultdict

# å‚æ•°è®¾ç½®ï¼ˆå¤ç”¨ä¹‹å‰çš„å®šä¹‰ï¼‰
SIZE = 9
ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]
ACTIONS_NAMES = ['â†‘', 'â†“', 'â†', 'â†’']
DOUBLE_ACTIONS={frozenset('â†‘â†“'):'â†•',frozenset('â†‘â†'):'â†‘â†',frozenset('â†‘â†’'):'â†‘â†’',
                frozenset('â†â†’'):'â†”',frozenset('â†â†“'):'â†â†“',frozenset('â†“â†’'):'â†“â†’'}
NUM_ACTIONS = len(ACTIONS)
GOAL_STATE= {80,10,50}
DANGER_STATES = {5,25,45,75}  # å±é™©åŒºåŸŸ
CHEST_STATES = {15:1,35:2,55:4}  # é”®ä¸ºçŠ¶æ€ç¼–å·ï¼Œå€¼ä¸ºå¯¹åº”çš„äºŒè¿›åˆ¶ä½
FULL_CHEST_MASK = sum(CHEST_STATES.values())
GAMMA = 0.9
NUM_EPISODES = 5000  # è®­ç»ƒè½®æ•°
EPSILON = 0.1  # æ¢ç´¢ç‡
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.995  # æ¯ä¸€è½®ä¹˜è¿™ä¸ªå› å­
THRESHOLD=1e-4
```

# äºŒ,å·¥å…·å‡½æ•°å’Œå¥–åŠ±å‡½æ•°å®šä¹‰


```python

def to_state(row, col):
    return row * SIZE + col

def from_state(s):
    return np.divmod(s, SIZE)
def is_terminal(s, mask):
    return s in GOAL_STATE and mask == FULL_CHEST_MASK

def step(s, mask, a):
    # ä¸å‰é¢ä¸€è‡´ï¼šè¿”å› next_s, next_mask, reward, done
    row, col = divmod(s, SIZE)
    dr, dc = ACTIONS[a]
    new_row = max(0, min(SIZE - 1, row + dr))
    new_col = max(0, min(SIZE - 1, col + dc))
    # if new_row==row and new_col==col:
    #     return s, mask, -100, False
    next_s = to_state(new_row, new_col)

    next_mask = mask
    reward =-1
    done = False

    if next_s in GOAL_STATE:
        done = True
        if mask == FULL_CHEST_MASK:
            reward = 1000
        else:
            reward = -100
            return s, mask, reward, done  # å›åŸåœ°

    elif next_s in DANGER_STATES:
        reward = -100

    elif next_s in CHEST_STATES:
        bit = CHEST_STATES[next_s]
        if not (next_mask & bit):
            next_mask = mask | bit
            reward = 50
        else:
            reward = -1

    return next_s, next_mask, reward, done

```

# ä¸‰,è¿­ä»£æ–¹æ³•

## 1.è’™ç‰¹å¡æ´›æ–¹æ³•

### 1.1 ç­–ç•¥ç”Ÿæˆ


```python

def generate_episode(policy, epsilon=EPSILON):
    # ç”Ÿæˆä¸€ä¸ª episode
    episode = []
    s = np.random.randint(SIZE * SIZE)
    mask = 0
    for _ in range(100):  # æœ€å¤§é•¿åº¦é™åˆ¶
        if is_terminal(s, mask):
            break
        # Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        if np.random.rand() < epsilon:
            a = np.random.choice(NUM_ACTIONS)
        else:
            a = policy[s, mask]
        next_s, next_mask, r, done = step(s, mask, a)
        episode.append((s, mask, a, r))
        s, mask = next_s, next_mask
    return episode

# åŠ¨æ€ Îµ-greedy ç­–ç•¥ç”Ÿæˆ episode
# ========================
def generate_episode_with_dynamic_epsilon(Q, epsilon):
    episode = []
    s = np.random.randint(SIZE * SIZE)
    mask = 0
    for _ in range(100):  # é˜²æ­¢æ­»å¾ªç¯
        if is_terminal(s, mask):
            break
        # Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        if np.random.rand() < epsilon:
            a = np.random.choice(NUM_ACTIONS)
        else:
            values = [Q.get((s, mask, ac), 0) for ac in range(NUM_ACTIONS)]
            a = np.argmax(values) if max(values)!=0 else np.random.choice(NUM_ACTIONS)
        next_s, next_mask, r, done = step(s, mask, a)
        episode.append((s, mask, a, r))
        s, mask = next_s, next_mask
    return episode
```

### 1.2 On-policy æ–¹æ³•


```python

def on_policy_monte_carlo_control():
    num_states = SIZE * SIZE
    num_masks = FULL_CHEST_MASK + 1

    # åˆå§‹åŒ– Q è¡¨å’Œç­–ç•¥
    Q = defaultdict(float)
    C = defaultdict(float)
    policy = np.zeros(shape=(num_states, num_masks), dtype=int)
    episode_returns = []
    for ep in range(NUM_EPISODES):
        episode = generate_episode(policy,max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep)))

        G = 0
        visited_sa = set()

        # ä»åå¾€å‰è®¡ç®— G
        for t in reversed(range(len(episode))):
            s, mask, a, r = episode[t]
            G = GAMMA * G + r
            sa = (s, mask, a)

            if sa not in visited_sa:
                visited_sa.add(sa)
                C[sa] += 1
                Q[sa] += (G - Q[sa]) / C[sa]  # å¢é‡å¹³å‡æ›´æ–°
        episode_returns.append(G)
        # ç­–ç•¥æ”¹è¿›ï¼ˆgreedyï¼‰
        for s in range(num_states):
            for mask in range(num_masks):
                if is_terminal(s, mask):
                    continue
                values = [Q.get((s, mask, a), 0) for a in range(NUM_ACTIONS)]
                best_a = np.argmax(values)
                policy[s, mask] = best_a

        if ep % 1000 == 0:
            print(f"Episode {ep} completed")

    return policy, Q , episode_returns
```

### 1.3 Off-policy æ–¹æ³•


```python

# ========================
# Off-policy First-Visit MC Control
# ========================
def off_policy_monte_carlo():
    num_states = SIZE * SIZE
    num_masks = FULL_CHEST_MASK + 1

    # åˆå§‹åŒ–
    Q = defaultdict(float)
    C = defaultdict(float)
    target_policy = np.random.choice(NUM_ACTIONS, size=(num_states, num_masks))
    episode_returns = []
    for ep in range(NUM_EPISODES):
        # åŠ¨æ€è°ƒæ•´ epsilon
        epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep))
        # ä½¿ç”¨å›ºå®šç­–ç•¥ Ï€_bï¼ˆbehavior policyï¼‰æ”¶é›†æ•°æ®
        episode = generate_episode_with_dynamic_epsilon(Q, epsilon)
        G = 0.0
        W = 1.0  # é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        visited_sa = set()

        # ä»åå¾€å‰å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥
        for t in reversed(range(len(episode))):
            s, mask, a, r = episode[t]
            G = GAMMA * G + r
            sa = (s, mask, a)

            if sa not in visited_sa:
                visited_sa.add(sa)

                # æ›´æ–°é‡è¦æ€§é‡‡æ ·ç»Ÿè®¡é‡
                C[sa] += W

                # å¢é‡å¹³å‡æ›´æ–° Q å€¼
                Q[sa] += (W / C[sa]) * (G - Q[sa])

                # æ›´æ–°ç›®æ ‡ç­–ç•¥ä¸º greedy
                values = [Q.get((s, mask, ac), 0) for ac in range(NUM_ACTIONS)]
                best_a = np.argmax(values)
                target_policy[s, mask] = best_a

                # å¦‚æœå½“å‰åŠ¨ä½œä¸æ˜¯ç›®æ ‡ç­–ç•¥é€‰çš„åŠ¨ä½œï¼Œåˆ™ç»ˆæ­¢æ›´æ–°è·¯å¾„
                if a != target_policy[s, mask]:
                    W = 1e-8  # æƒé‡å½’é›¶ï¼Œä¸å†æ›´æ–°å‰é¢çš„çŠ¶æ€
                    break
            else:
                continue
        episode_returns.append(G)
        if ep % 500 == 0:
            print(f"Episode {ep} completed")

    return target_policy, Q,  episode_returns
```

## 2.åŠ¨æ€è§„åˆ’æ–¹æ³•

### 2.1  ç­–ç•¥è¯„ä¼°


```python

def policy_evaluation(policy,V,env_step=step):
    value_deltas = []
    while True:
        delta = 0
        V_new=np.zeros_like(V)
        for s in range(SIZE*SIZE):
            for mask in range(FULL_CHEST_MASK + 1):
                if is_terminal(s, mask):  # å¦‚æœæ˜¯ç»ˆç‚¹ä¸”å®ç®±é›†é½
                    continue
                v = V[s, mask]
                a = policy[s, mask]

                # ç¯å¢ƒè½¬ç§»å‡½æ•°åº”æ”¹ä¸ºæ¥å— (s, mask, a) è¿”å› (ns, nmask, r, done)
                ns, nmask, r, done = env_step(s, mask, a)
                V_new[s,mask]=r + GAMMA * V[ns, nmask]
                delta = max(delta, abs(v - V_new[s, mask]))
        value_deltas.append(delta)
        V = V_new
        if delta < THRESHOLD:
            break

    return V, value_deltas
def policy_improvement(policy,V,env_step=step):
    stable = True
    for s in range(SIZE*SIZE):
        for mask in range(FULL_CHEST_MASK+1):
            if is_terminal(s, mask):
                continue
            old_action = policy[s, mask]
            values = []
            for a in range(NUM_ACTIONS):
                ns, nmask, r, done = env_step(s, mask, a)
                values.append(r + GAMMA * V[ns, nmask])
            best_a = np.argmax(values)
            # å…³é”®ç‚¹ ä¸¤ä¸ªå€¼ç›¸å·®ä¸å¤§ä¼šå½±å“ ç­–ç•¥è¯„ä¼°,ä»è€Œå¯¼è‡´éœ‡è¡
            if best_a != old_action and values[best_a] - values[old_action] >0.2:
                policy[s, mask] = best_a
                stable = False
    return policy, stable
def policy_iteration():
    # åˆå§‹åŒ–ç­–ç•¥å’Œä»·å€¼å‡½æ•°ï¼ˆç°åœ¨æ˜¯äºŒç»´ï¼‰
    # æ‰©å±•çŠ¶æ€æ€»æ•° = åŸå§‹çŠ¶æ€æ•° Ã— å®ç®±æ©ç æ•°é‡
    num_states = SIZE * SIZE
    num_masks = FULL_CHEST_MASK + 1  # ä» 0 åˆ° FULL_CHEST_MASK
    policy = np.zeros(shape=(num_states, num_masks), dtype=int)
    V = np.zeros(shape=(num_states, num_masks))
    deltas_history = []
    for i in range(100):
        V,deltas=policy_evaluation(policy,V)
        deltas_history.extend(deltas)
        policy,stable=policy_improvement(policy,V)
        if stable:
            print(f'Policy iteration converged at iteration {i}')
            break
    return policy,V,deltas_history
```

### 2.2 ä»·å€¼è¿­ä»£


```python

def value_iteration():
    num_state=SIZE*SIZE
    V=np.zeros(shape=(num_state,FULL_CHEST_MASK+1))
    policy=np.zeros(shape=(num_state,FULL_CHEST_MASK+1),dtype=int)
    deltas_history=[]
    while True:
        delta=0
        V_new=np.zeros_like(V)
        for s in range(num_state):
            for mask  in range(FULL_CHEST_MASK+1):
                if is_terminal(s,mask):
                    continue
                values=[]
                for a in range(NUM_ACTIONS):
                    next_state,next_mask,reward,done=step(s,mask,a)
                    if not done:
                        values.append(reward+GAMMA*V[next_state,next_mask])
                    else:
                        values.append(reward)
                new_value=max(values)
                delta=max(delta,abs(new_value-V[s,mask]))
                V_new[s,mask]=new_value
        deltas_history.append(delta)
        V=V_new
        if delta<THRESHOLD:
            print(f'Value iteration converged at iteration {len(deltas_history)}')
            break

    for s in range(num_state):
        for mask in range(FULL_CHEST_MASK+1):
            if is_terminal(s,mask):
                policy[s,mask]=-1
                continue
            values=[]
            for a in range(NUM_ACTIONS):
                next_state,next_mask,reward,done=step(s,mask,a)
                values.append(reward+GAMMA*V[next_state,next_mask])
            policy[s,mask]=np.argmax(values)
    return policy,V,deltas_history
```

## 3.æ—¶åˆ†åºåˆ—æ–¹æ³•

###  3.1 Q-learning


```python
def q_learing():
    import numpy as np
    # å‡è®¾ä½ æœ‰ SIZE*SIZE ä¸ªçŠ¶æ€ï¼ŒFULL_CHEST_MASK+1 ç§ maskï¼ŒNUM_ACTIONS ä¸ªåŠ¨ä½œ
    Q = np.zeros(shape=(SIZE*SIZE, FULL_CHEST_MASK+1, NUM_ACTIONS))
    GAMMA = 0.95     # æŠ˜æ‰£å› å­
    ALPHA = 0.1      # å­¦ä¹ ç‡
    EPSILON_START = 0.9
    EPSILON_END = 0.1
    EPSILON_DECAY = 0.995
    EPISODES = 5000 # æ€»å…±è®­ç»ƒå¤šå°‘ episode
    MAX_STEPS = 100  # æ¯ä¸ª episode æœ€å¤§æ­¥æ•°
    def choose_action(state, mask, Q, epsilon):
        if np.random.rand() < epsilon:
            return np.random.randint(NUM_ACTIONS)
        else:
            return np.argmax(Q[state, mask])
    def enviroment_reset():
        return np.random.randint(SIZE*SIZE)
    def initial_mask(initial_state):
        return 0 if initial_state not in CHEST_STATES else CHEST_STATES[initial_state]
    gradient_history = []
    for episode in range(EPISODES):
        state = enviroment_reset()
        mask = initial_mask(state)
        delta=0
        for _ in range(MAX_STEPS):
            if state in GOAL_STATE:
                break
            epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** episode))
            action = choose_action(state, mask, Q, epsilon)

            # ä¸ç¯å¢ƒäº¤äº’å¾—åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€ç­‰ä¿¡æ¯
            next_state, next_mask, reward, done = step(state,mask,action)

            # Q-learning æ›´æ–°å…¬å¼
            td_target = (reward + GAMMA * np.max(Q[next_state, next_mask]) )if not done else reward
            td_error = td_target - Q[state, mask, action]
            Q[state, mask, action] += ALPHA * td_error
            delta=max(delta,abs(td_error))
            state = next_state
            mask = next_mask
            if done:
                break
        gradient_history.append(delta)
    return np.argmax(Q,  axis=2),  Q, gradient_history
```

### 3.2 SARSA


```python

def sarsa():
    import numpy as np
    from collections import defaultdict

    # è¶…å‚æ•°
    NUM_EPISODES = 5000
    GAMMA = 0.99
    ALPHA = 0.1      # å­¦ä¹ ç‡
    EPSILON_START = 0.5
    EPSILON_END = 0.01
    EPSILON_DECAY = 0.995
    num_states = SIZE * SIZE
    num_masks = FULL_CHEST_MASK + 1
    def enviroment_reset():
        state=np.random.randint(SIZE*SIZE)
        return state,0 if state not in CHEST_STATES else CHEST_STATES[state]
    # åˆå§‹åŒ– Q è¡¨æ ¼
    Q = np.zeros(shape=(num_states, num_masks, NUM_ACTIONS))
    episode_returns = []

    for ep in range(NUM_EPISODES):
        epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep))

        # é‡ç½®ç¯å¢ƒï¼ˆä½ éœ€è¦è‡ªå·±å®ç° reset å‡½æ•°ï¼‰
        state, mask = enviroment_reset()  # è¿”å›åˆå§‹ state å’Œ mask

        # Îµ-greedy é€‰æ‹©ç¬¬ä¸€ä¸ªåŠ¨ä½œ
        if np.random.rand() < epsilon:
            action = np.random.choice(NUM_ACTIONS)
        else:
            action = np.argmax(Q[(state, mask)])

        done = state in GOAL_STATE
        total_return = 0

        while not done:
            # æ‰§è¡Œå½“å‰åŠ¨ä½œï¼Œå¾—åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±ç­‰
            next_state, next_mask, reward, done = step(state, mask, action)

            # Îµ-greedy é€‰æ‹©ä¸‹ä¸€ä¸ªåŠ¨ä½œ
            if np.random.rand() < epsilon:
                next_action = np.random.choice(NUM_ACTIONS)
            else:
                next_action = np.argmax(Q[(next_state, next_mask)])

            # SARSA æ›´æ–°å…¬å¼
            td_target = reward + GAMMA * Q[(next_state, next_mask)][next_action]
            td_error = td_target - Q[(state, mask)][action]
            Q[(state, mask)][action] += ALPHA * td_error

            # ç´¯è®¡å›æŠ¥
            total_return += td_error

            # æ›´æ–°çŠ¶æ€å’ŒåŠ¨ä½œ
            state, mask = next_state, next_mask
            action = next_action

        episode_returns.append(total_return)

        if ep % 500 == 0:
            print(f"Episode {ep} completed")

    return np.argmax(Q,  axis=2),Q, episode_returns
```

### 3.3 N-Sarsa


```python
def n_step_sarsa(n=3, NUM_EPISODES=5000):
    import numpy as np
    from collections import defaultdict

    # è¶…å‚æ•°
    GAMMA = 0.99
    ALPHA = 0.1      # å­¦ä¹ ç‡
    EPSILON_START = 0.5
    EPSILON_END = 0.01
    EPSILON_DECAY = 0.995
    num_states = SIZE * SIZE
    num_masks = FULL_CHEST_MASK + 1
    MAX_EPISODE_LENGTH = num_states+1  # è®¾ç½®æœ€å¤§æ­¥æ•°
    def enviroment_reset():
        state=np.random.randint(SIZE*SIZE)
        return state,0 if state not in CHEST_STATES else CHEST_STATES[state]
    # åˆå§‹åŒ– Q è¡¨æ ¼ï¼šQ[state][mask][action]
    Q = np.zeros(shape=(num_states, num_masks, NUM_ACTIONS))

    episode_returns = []

    for ep in range(NUM_EPISODES):
        epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep))

        # åˆå§‹åŒ–ç¯å¢ƒ
        state, mask = enviroment_reset()  # è¿”å›åˆå§‹ state å’Œ mask
        done = state in GOAL_STATE

        # Îµ-greedy é€‰æ‹©ç¬¬ä¸€ä¸ªåŠ¨ä½œ
        if np.random.rand() < epsilon:
            action = np.random.choice(NUM_ACTIONS)
        else:
            action = np.argmax(Q[state, mask])

        # å­˜å‚¨è½¨è¿¹ä¿¡æ¯
        states = [state]
        masks = [mask]
        actions = [action]
        rewards = []

        T = float('inf')  # episode çš„æ€»æ­¥æ•°ï¼ˆè¿˜æœªçŸ¥ï¼‰
        t = 0  # å½“å‰æ—¶é—´æ­¥
        # ç´¯è®¡æœ¬ episode çš„å›æŠ¥
        total_return = 0
        while True:
            if t < T:
                # æ‰§è¡ŒåŠ¨ä½œï¼Œå¾—åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±ç­‰
                next_state, next_mask, reward, done = step(state, mask, action)
                next_action=None
                # å­˜å‚¨å¥–åŠ±
                rewards.append(reward)

                # å¦‚æœæœªç»“æŸï¼Œç»§ç»­é€‰åŠ¨ä½œ
                if not done or t < MAX_EPISODE_LENGTH:
                    if np.random.rand() < epsilon:
                        next_action = np.random.choice(NUM_ACTIONS)
                    else:
                        next_action = np.argmax(Q[next_state, next_mask])
                    # å­˜å‚¨ä¸‹ä¸€ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œ
                    states.append(next_state)
                    masks.append(next_mask)
                    actions.append(next_action)
                else:
                    T = t + 1  # è®¾ç½® episode ç»“æŸæ—¶é—´

                # æ›´æ–°çŠ¶æ€å’ŒåŠ¨ä½œ
                state, mask, action = next_state, next_mask, next_action

            # æ›´æ–°ç›®æ ‡æ—¶é—´ç‚¹ Ï„
            tau = t - n + 1
            if tau >= 0:
                # è®¡ç®— G_t^(n)ï¼šn-step return
                end = min(tau + n, T)
                G = sum([GAMMA ** (k - tau) * rewards[k] for k in range(tau, end)])

                if end < T:
                    G += GAMMA ** n * Q[states[end], masks[end]][actions[end]]
                delta=ALPHA * (
                    G - Q[states[tau], masks[tau], actions[tau]]
                )
                # æ›´æ–° Q å€¼
                Q[states[tau], masks[tau], actions[tau]] += delta
                total_return += delta/len(states)

            if tau == T - 1:
                break

            t += 1

        episode_returns.append(total_return)

        if ep % 500 == 0:
            print(f"Episode {ep} completed")

    # è¿”å›æœ€ä¼˜ç­–ç•¥ï¼ˆå– argmaxï¼‰å’Œ Q è¡¨
    return np.argmax(Q, axis=2), Q, episode_returns
```

# å››,è¾“å‡ºç­–ç•¥å’Œä»·å€¼


```python

def print_policy(policy,mask=0):
    for i in range(SIZE):
        row = ""
        for j in range(SIZE):
            s = to_state(i, j)
            if s in GOAL_STATE:
                cell = "G"
            elif s in DANGER_STATES:
                cell = "X"
            elif s in CHEST_STATES:
                cell = CHEST_STATES[s]
            else:
                cell = ACTIONS_NAMES[policy[s, mask]]
            row += f"{cell:^5}"
        print(row)
def print_values(V,mask=0):
    for i in range(SIZE):
        row = ""
        for j in range(SIZE):
            s = to_state(i, j)
            # if s in GOAL_STATE:
            #     cell = 'G'
            # elif s in DANGER_STATES:
            #     cell = 'X'
            # elif s in CHEST_STATES:
            #     cell = f"$"
            # else:
            cell = f"{V[s,mask]:.2f}"
            row += f"{cell:^7}"  # å±…ä¸­å¯¹é½ï¼Œæ¯ä¸ªæ•°å­—å 7ä¸ªå­—ç¬¦å®½åº¦
        print(row)
def print_environment():
    for i in range(SIZE):
        row = ""
        for j in range(SIZE):
            s = to_state(i, j)
            cell = "0"
            if s in GOAL_STATE:
                cell = 'G'
            elif s in DANGER_STATES:
                cell = 'X'
            elif s in CHEST_STATES:
                cell = f"$"
            else:
                pass
            row += f"{cell:^7}"  # å±…ä¸­å¯¹é½ï¼Œæ¯ä¸ªæ•°å­—å 7ä¸ªå­—ç¬¦å®½åº¦
        print(row)
def print_path(x,y,policy):
    path=[ ["0"  for _ in range(SIZE)] for _  in range(SIZE)]
    state=to_state(x,y)
    mask=0
    if state in CHEST_STATES:
        mask=CHEST_STATES[state]
    times=0
    while state not in GOAL_STATE:
        times+=1
        action=policy[state,mask]
        next_state,mask,_,_=step(state,mask,action)
        i,j=from_state(state)
        actions=frozenset(ACTIONS_NAMES[action]+path[i][j])
        path[i][j]=DOUBLE_ACTIONS.get(actions,ACTIONS_NAMES[action])
        if state>=SIZE*SIZE or state<0 or state==next_state or times>SIZE*SIZE:
            break
        state=next_state
    i,j=from_state(state)
    path[i][j]="ğŸ"
    path[x][y]="ğŸ"
    for i in range(SIZE):
        for j in range(SIZE):
            path[i][j]=path[i][j].center(7)
    print(f"\n Start from {x,y} to Goal")
    for row in path:
        print("".join(row))
def moving_average(a, window_size=50):
    return np.convolve(a, np.ones(window_size)/window_size, mode='valid')
def show_gradient_history(gradient_history):
    import matplotlib.pyplot as plt
    plt.figure(figsize=(12,8))
    plt.plot(moving_average(gradient_history))
    plt.xlabel('Episode')
    plt.ylabel('TD Error')
    plt.title('TD Error History')
    plt.show()

```

# äº”,è°ƒè¯•


```python
print("Environment:")
print_environment()

on_policy,on_Q,on_episode_returns=on_policy_monte_carlo_control()
print("\n On Policy:")
print_policy(on_policy)

off_policy, off_Q,off_episode_returns = off_policy_monte_carlo()
print("\n Off Policy:")
print_policy(off_policy)

policy,V,deltas_history=policy_iteration()
print("\n Policy Iteration:")
print_policy(policy)

value_policy,value_V,value_deltas_history=value_iteration()
print("\n Value Iteration:")
print_policy(value_policy)
q_policy,Q,gradient_history=q_learing()
print("\n Q-Learning:")
print_policy(q_policy)
print("\n SARSA")
sarsa_policy,sarsa_Q,sarsa_gradient_history=sarsa()
print_policy(sarsa_policy)
print("\n N-Step SARSA")
n_sarsa_policy,n_sarsa_Q,n_sarsa_gradient_history=n_step_sarsa()
print_policy(n_sarsa_policy)
```

    Environment:
       0      0      0      0      0      X      0      0      0   
       0      G      0      0      0      0      $      0      0   
       0      0      0      0      0      0      0      X      0   
       0      0      0      0      0      0      0      0      $   
       0      0      0      0      0      0      0      0      0   
       X      0      0      0      0      G      0      0      0   
       0      $      0      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
       0      0      0      X      0      0      0      0      G   
    Episode 0 completed
    Episode 1000 completed
    Episode 2000 completed
    Episode 3000 completed
    Episode 4000 completed
    
     On Policy:
      â†’    â†’    â†’    â†“    â†    X    â†’    â†’    â†“  
      â†‘    G    â†“    â†’    â†’    â†’    1    â†’    â†“  
      â†’    â†“    â†“    â†“    â†‘    â†‘    â†‘    X    â†“  
      â†’    â†’    â†’    â†’    â†‘    â†‘    â†’    â†’    2  
      â†‘    â†’    â†’    â†“    â†    â†‘    â†‘    â†    â†‘  
      X    â†’    â†“    â†    â†    G    â†’    â†‘    â†  
      â†’    4    â†    â†’    â†    â†    â†’    â†‘    â†‘  
      â†’    â†‘    â†    â†    â†    â†    â†‘    â†‘    â†‘  
      â†‘    â†’    â†‘    X    â†‘    â†“    â†‘    â†    G  
    Episode 0 completed
    Episode 500 completed
    Episode 1000 completed
    Episode 1500 completed
    Episode 2000 completed
    Episode 2500 completed
    Episode 3000 completed
    Episode 3500 completed
    Episode 4000 completed
    Episode 4500 completed
    
     Off Policy:
      â†’    â†’    â†’    â†’    â†’    X    â†“    â†    â†  
      â†‘    G    â†“    â†    â†’    â†’    1    â†‘    â†“  
      â†‘    â†’    â†“    â†“    â†‘    â†    â†‘    X    â†“  
      â†“    â†“    â†    â†’    â†‘    â†’    â†“    â†’    2  
      â†’    â†’    â†’    â†‘    â†‘    â†’    â†’    â†’    â†‘  
      X    â†‘    â†‘    â†‘    â†    G    â†‘    â†‘    â†‘  
      â†’    4    â†’    â†“    â†’    â†’    â†‘    â†’    â†‘  
      â†’    â†’    â†’    â†’    â†’    â†‘    â†‘    â†’    â†‘  
      â†’    â†‘    â†‘    X    â†’    â†‘    â†    â†‘    G  
    Policy iteration converged at iteration 13
    
     Policy Iteration:
      â†’    â†’    â†’    â†’    â†“    X    â†“    â†“    â†  
      â†“    G    â†’    â†’    â†’    â†’    1    â†    â†  
      â†“    â†“    â†‘    â†‘    â†‘    â†‘    â†‘    X    â†“  
      â†“    â†“    â†    â†‘    â†‘    â†‘    â†‘    â†’    2  
      â†’    â†“    â†“    â†“    â†‘    â†‘    â†‘    â†’    â†‘  
      X    â†“    â†“    â†“    â†“    G    â†‘    â†‘    â†‘  
      â†’    4    â†    â†    â†    â†    â†‘    â†‘    â†‘  
      â†‘    â†‘    â†‘    â†    â†    â†    â†‘    â†‘    â†‘  
      â†’    â†‘    â†‘    X    â†‘    â†‘    â†’    â†‘    G  
    Value iteration converged at iteration 27
    
     Value Iteration:
      â†’    â†’    â†“    â†“    â†“    X    â†“    â†“    â†“  
      â†“    G    â†’    â†’    â†’    â†’    1    â†    â†“  
      â†“    â†“    â†‘    â†‘    â†‘    â†‘    â†‘    X    â†“  
      â†“    â†“    â†“    â†‘    â†‘    â†‘    â†‘    â†’    2  
      â†’    â†“    â†“    â†“    â†‘    â†‘    â†‘    â†‘    â†‘  
      X    â†“    â†“    â†“    â†“    G    â†‘    â†‘    â†‘  
      â†’    4    â†    â†    â†    â†    â†‘    â†‘    â†‘  
      â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘  
      â†‘    â†‘    â†‘    X    â†‘    â†‘    â†‘    â†‘    G  
    
     Q-Learning:
      â†’    â†’    â†“    â†’    â†“    X    â†“    â†“    â†  
      â†‘    G    â†’    â†’    â†’    â†’    1    â†    â†  
      â†“    â†“    â†‘    â†’    â†‘    â†‘    â†‘    X    â†“  
      â†“    â†’    â†“    â†    â†‘    â†‘    â†‘    â†’    2  
      â†’    â†“    â†“    â†“    â†    â†‘    â†’    â†’    â†‘  
      X    â†“    â†“    â†    â†    G    â†‘    â†‘    â†‘  
      â†’    4    â†    â†    â†    â†    â†    â†‘    â†‘  
      â†‘    â†‘    â†‘    â†    â†‘    â†    â†    â†‘    â†‘  
      â†’    â†‘    â†‘    X    â†‘    â†‘    â†    â†‘    G  
    
     SARSA
    Episode 0 completed
    Episode 500 completed
    Episode 1000 completed
    Episode 1500 completed
    Episode 2000 completed
    Episode 2500 completed
    Episode 3000 completed
    Episode 3500 completed
    Episode 4000 completed
    Episode 4500 completed
      â†“    â†’    â†“    â†’    â†“    X    â†“    â†“    â†  
      â†“    G    â†’    â†’    â†’    â†’    1    â†    â†  
      â†“    â†“    â†‘    â†‘    â†‘    â†’    â†‘    X    â†“  
      â†’    â†“    â†    â†    â†’    â†’    â†‘    â†’    2  
      â†’    â†“    â†“    â†“    â†    â†‘    â†‘    â†’    â†‘  
      X    â†“    â†    â†“    â†    G    â†’    â†’    â†‘  
      â†’    4    â†    â†    â†    â†    â†’    â†’    â†‘  
      â†’    â†‘    â†‘    â†    â†‘    â†    â†    â†’    â†‘  
      â†’    â†‘    â†‘    X    â†‘    â†    â†‘    â†‘    G  
    
     N-Step SARSA
    Episode 0 completed
    Episode 500 completed
    Episode 1000 completed
    Episode 1500 completed
    Episode 2000 completed
    Episode 2500 completed
    Episode 3000 completed
    Episode 3500 completed
    Episode 4000 completed
    Episode 4500 completed
      â†“    â†    â†    â†    â†“    X    â†“    â†    â†  
      â†“    G    â†“    â†“    â†’    â†“    1    â†‘    â†  
      â†“    â†’    â†’    â†“    â†“    â†“    â†‘    X    â†“  
      â†’    â†’    â†“    â†’    â†’    â†“    â†    â†’    2  
      â†‘    â†    â†“    â†“    â†’    â†’    â†“    â†’    â†‘  
      X    â†“    â†    â†    â†    G    â†’    â†’    â†‘  
      â†’    4    â†‘    â†    â†    â†    â†’    â†’    â†‘  
      â†‘    â†‘    â†    â†’    â†‘    â†    â†‘    â†’    â†‘  
      â†’    â†‘    â†    X    â†‘    â†’    â†‘    â†    G  
    


```python
import matplotlib.pyplot as plt

plt.figure(figsize=(12,8))
plt.plot(moving_average(on_episode_returns), label='On-policy')
plt.plot(moving_average(off_episode_returns), label='Off-policy')
plt.xlabel('Episode')
plt.ylabel('Average Return')
plt.legend()

plt.show()
```


    
![png](output_28_0.png)
    



```python
plt.figure(figsize=(12,8))
plt.plot(value_deltas_history, label='value-policy')
plt.plot(deltas_history, label='policy')
plt.xlabel('Episode')
plt.ylabel('Delta Decay')
plt.legend()
plt.show()
```


    
![png](output_29_0.png)
    



```python
plt.figure(figsize=(12,8))
plt.plot(moving_average(gradient_history), label='Q-Learning')
plt.plot(moving_average(sarsa_gradient_history), label='SARSA')
plt.plot(moving_average(n_sarsa_gradient_history), label='N-Step SARSA')
plt.xlabel('Episode')
plt.ylabel('TD Error')
plt.legend()
plt.show()
```


    
![png](output_30_0.png)
    



```python
start_pos=(0,0)
print("\n On Policy")
print_path(*start_pos,on_policy)
print("\n Off Policy")
print_path(*start_pos,off_policy)
print("\n Policy Iteration")
print_path(*start_pos,policy)
print("\n Value Iteration")
print_path(*start_pos,value_policy)
print("\n Q-Learning")
print_path(*start_pos,q_policy)
print("\n SARSA")
print_path(*start_pos,sarsa_policy)
print("\n N-Step SARSA")
print_path(*start_pos,n_sarsa_policy)
```

    
     On Policy
    
     Start from (0, 0) to Goal
       ğŸ      â†’      â†’      â†“      0      0      0      0      0   
       0      0      0      â†’      â†’      â†’      â†’      â†’      â†“   
       0      0      0      0      0      0      0      0      â†“   
       0      0      0      0      0      0      0      0      â†“   
       0      0      0      0      0      0      0      â†“      â†   
       0      0      0      0      0      ğŸ      0      â†“      0   
       0      â†’      â†’      â†’      â†’      â†‘      â†“      â†      0   
       0      â†‘      â†      â†      â†      â†      â†      0      0   
       0      0      0      0      0      0      0      0      0   
    
     Off Policy
    
     Start from (0, 0) to Goal
       ğŸ      â†’      â†’      â†’      â†’      â†’      â†“      0      0   
       0      0      0      0      0      0      â†’      â†’      â†“   
       0      0      0      0      0      0      0      0      â†“   
       0      0      0      0      0      0      0      0      â†“   
       0      0      0      0      0      0      0      0      â†“   
       0      0      â†’      â†’      â†’      ğŸ      0      â†“      â†   
       0      â†’      â†‘â†     â†      â†      â†      â†      â†      0   
       0      0      0      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
    
     Policy Iteration
    
     Start from (0, 0) to Goal
       ğŸ      â†’      â†’      â†’      â†“      0      0      0      0   
       0      ğŸ      0      0      â†’      â†’      â†“      0      0   
       0      â†‘      0      0      0      0      â†“      0      0   
       0      â†‘      0      0      0      0      â†’      â†’      â†“   
       0      â†‘      0      0      0      0      0      0      â†“   
       0      â†‘      0      0      0      0      0      0      â†“   
       0      â†‘      â†      â†      â†      â†      â†      â†      â†   
       0      0      0      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
    
     Value Iteration
    
     Start from (0, 0) to Goal
       ğŸ      â†’      â†“      0      0      0      0      0      0   
       0      ğŸ      â†’      â†’      â†’      â†’      â†“      0      0   
       0      â†‘      0      0      0      0      â†“      0      0   
       0      â†‘      0      0      0      0      â†’      â†’      â†“   
       0      â†‘      0      0      0      0      0      0      â†“   
       0      â†‘      0      0      0      0      0      0      â†“   
       0      â†‘      â†      â†      â†      â†      â†      â†      â†   
       0      0      0      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
    
     Q-Learning
    
     Start from (0, 0) to Goal
       ğŸ      â†’      â†“      0      0      0      0      0      0   
       0      0      â†’      â†’      â†’      â†’      â†’      â†’      â†“   
       0      0      0      0      0      0      0      0      â†“   
       0      0      0      â†“      â†      â†      â†      â†      â†   
       0      0      â†“      â†      0      0      0      0      0   
       0      â†“â†’     â†”      â†’      â†’      ğŸ      0      0      0   
       0      â†‘      0      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
    
     SARSA
    
     Start from (0, 0) to Goal
       ğŸ      0      0      0      0      0      0      0      0   
       â†“      0      0      0      0      0      â†’      â†’      â†“   
       â†“      0      0      0      0      0      â†‘      0      â†“   
       â†’      â†“      0      0      â†’      â†’      â†‘      0      â†“   
       0      â†“      0      â†’      â†‘      â†“      â†      â†      â†   
       0      â†“      â†’      â†‘      0      ğŸ      0      0      0   
       0      â†’      â†‘      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
    
     N-Step SARSA
    
     Start from (0, 0) to Goal
       ğŸ      0      0      0      0      0      0      0      0   
       â†“      0      0      0      0      0      â†“      â†      â†   
       â†“      0      0      0      0      0      â†“      0      â†‘   
       â†’      â†’      â†“      0      â†’      â†’      â†“â†’     â†’      â†‘   
       0      0      â†“      â†’      â†‘      0      â†“      0      0   
       0      â†“      â†”      â†‘      0      ğŸ      â†      0      0   
       0      â†’      â†‘      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
    

# å…­,ç½‘ç»œæ¨¡å‹


```python
import  torch
import  torch.nn as nn
import  torch.optim as optim
```


```python
a=torch.randint(0,10,(10,),dtype=torch.float32)
net=torch.nn.Linear(10,5)
b=net(torch.tensor([1]*10,dtype=torch.float32))
print(b)
```

    tensor([ 0.5811, -0.2161,  0.4089,  0.6695, -0.6427], grad_fn=<ViewBackward0>)
    


```python
class DQN(nn.Module):
    def __init__(self, input_dim, num_actions):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, num_actions)
        )

    def forward(self, x):
        return self.net(x)
```


```python
import random
from collections import  deque
class DQNAgent:
    def __init__(self, input_dim, num_actions):
        self.num_actions = num_actions
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.policy_net = DQN(input_dim, num_actions).to(self.device)
        self.target_net = DQN(input_dim, num_actions).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3,  weight_decay=1e-5)
        # self.scheduler=  optim.lr_scheduler.ReduceLROnPlateau(
        #                     optimizer=self.optimizer,
        #                     mode='min',        # ç›‘æ§çš„æŒ‡æ ‡æ˜¯ lossï¼Œè¶Šå°è¶Šå¥½
        #                     factor=0.5,        # å­¦ä¹ ç‡ä¹˜ä»¥ 0.5
        #                     patience=10,       # ç­‰å¾… 10 ä¸ª episode loss æ²¡æœ‰ä¸‹é™æ‰è§¦å‘è¡°å‡
        #                     min_lr=1e-5        # æœ€å°å­¦ç‡é™åˆ¶
        #                 )
        self.memory = deque(maxlen=10000)
        self.batch_size = 64
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.1
        self.epsilon_decay = 0.995

    def select_action(self, state):

        if np.random.rand() < self.epsilon:
            return np.random.randint(self.num_actions)
        else:
            with torch.no_grad():
                state = torch.FloatTensor(state).to(self.device)
                q_values = self.policy_net(state)
                return q_values.argmax().item()

    def store_transition(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def update(self):
        if len(self.memory) < self.batch_size:
            return

        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        current_q = self.policy_net(states).gather(1, actions).squeeze()
        next_q = self.target_net(next_states).max(1)[0]
        expected_q = rewards + (1 - dones) * self.gamma * next_q

        loss = nn.MSELoss()(current_q, expected_q.detach())
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        # ğŸ‘‡ å°† loss ä¼ ç»™ scheduler
        # self.scheduler.step(loss.item())

        # # Epsilon decay
        # self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.soft_update_target()
        return loss

    def soft_update_target(self, tau=0.01):
        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):
            target_param.data.copy_(tau * policy_param.data + (1 - tau) * target_param.data)
```


```python

def enviroment_reset():
    state=np.random.randint(SIZE*SIZE)
    mask=0 if state not in CHEST_STATES  else CHEST_STATES[state]
    return state,mask
def toTensor(state,mask):
    state_onehot = np.zeros(SIZE * SIZE)
    state_onehot[state] = 1
    bin_mask=list(map(int,bin(mask)[2:].zfill(len(CHEST_STATES))))
    mask_onehot = np.array(bin_mask)
    return torch.tensor(np.concatenate([state_onehot, mask_onehot]), dtype=torch.float32)
def train_dqn(episodes=1000, input_dim=84, num_actions=4):
    agent = DQNAgent(input_dim, num_actions)
    rewards_history = []

    for ep in range(episodes):
        state,mask=enviroment_reset()
        total_reward = 0
        total_loss=0
        tensor=toTensor(state,mask)
        done = state in GOAL_STATE
        if done:
            continue
        path_length=0
        while not done and path_length<81:
            path_length+=1
            action = agent.select_action(tensor)
            next_state,next_mask, reward, done= step(state,mask,action)
            next_tensor=toTensor(next_state,next_mask)
            agent.store_transition(tensor, action, reward, next_tensor, done)
            loss=agent.update()
            total_loss+=loss.item()  if loss is not None else 0
            total_reward += reward
            state = next_state
            mask=next_mask
            tensor=next_tensor
        avg_loss=total_loss/path_length
        rewards_history.append(avg_loss)
        agent.epsilon=max(agent.epsilon*agent.epsilon_decay,agent.epsilon_min)
        print(f"\rEpisode {ep+1}, Total Reward: {total_reward/path_length:.2f} Total Loss: {avg_loss:.2f}, Epsilon: {agent.epsilon:.2f}", end="")
        if 0<avg_loss<=0.01:
            break
    return agent, rewards_history
```


```python
agent,  rewards_history = train_dqn()
```

    Episode 1000, Total Reward: 37.47 Total Loss: 7.97, Epsilon: 0.100118


```python
import matplotlib.pyplot as plt
plt.plot(moving_average(rewards_history))
plt.xlabel("Episode")
plt.ylabel("Total Loss")
plt.title("Training Progress")
plt.show()
```


    
![png](output_39_0.png)
    



```python
policy=np.zeros(shape=(SIZE*SIZE,FULL_CHEST_MASK+1))
agent.target_net.eval()
for i in range(SIZE*SIZE):
    for j in range(FULL_CHEST_MASK+1):
        policy[i][j]=np.argmax(agent.target_net(toTensor(i,j).to('cuda')).cpu().detach().numpy())
```


```python
print_path(0,0,np.array(policy,dtype=int))
```

    
     Start from (0, 0) to Goal
       ğŸ      â†’      â†“      0      0      0      0      0      0   
       0      0      â†’      â†’      â†’      â†’      â†“      0      0   
       0      0      0      0      0      0      â†“      0      0   
       0      0      0      0      0      0      â†’      â†’      â†“   
       0      0      0      0      0      0      0      0      â†“   
       0      0      0      0      0      ğŸ      â†“      â†      â†   
       0      â†’      â†”      â†”      â†”      â†‘â†     â†      0      0   
       0      0      0      0      0      0      0      0      0   
       0      0      0      0      0      0      0      0      0   
    


```python
print_policy(np.array(policy,dtype=int))
```

      â†’    â†’    â†“    â†’    â†“    X    â†“    â†    â†  
      â†“    G    â†’    â†’    â†’    â†’    1    â†    â†  
      â†’    â†“    â†‘    â†’    â†’    â†’    â†‘    X    â†“  
      â†“    â†“    â†“    â†’    â†‘    â†‘    â†‘    â†’    2  
      â†’    â†“    â†    â†“    â†‘    â†’    â†’    â†’    â†‘  
      X    â†“    â†    â†    â†“    G    â†’    â†’    â†‘  
      â†’    4    â†    â†    â†    â†    â†’    â†’    â†‘  
      â†’    â†‘    â†‘    â†    â†    â†    â†‘    â†‘    â†‘  
      â†‘    â†‘    â†    X    â†‘    â†    â†’    â†‘    G  
    


```python
# torch.save(agent.target_net.state_dict(), "model.pt")
```


```python

```
